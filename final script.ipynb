{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d189a067",
   "metadata": {},
   "source": [
    "# install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377f797a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import folium\n",
    "import dataframe_image as dfi\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics import silhouette_score as ss\n",
    "import itertools\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from matplotlib import pyplot as plt\n",
    "import random\n",
    "from shapely.geometry import MultiPoint\n",
    "from geopy.distance import great_circle\n",
    "from sklearn.metrics import davies_bouldin_score\n",
    "from sklearn.metrics import calinski_harabasz_score\n",
    "import mmh3\n",
    "from nltk.corpus import stopwords\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1fe19ac",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11cd9376",
   "metadata": {},
   "source": [
    "# Data processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051ee6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data\n",
    "df = pd.read_csv('../../US_Accidents_May19_Migrated Data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f6f122",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset observing\n",
    "df.shape\n",
    "df.head()\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637b1e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting attribute\n",
    "df.drop = df[['ID','City','State','Severity','Visibility(mi)','Start_Lat','Start_Lng', \n",
    "            'count Traffic Signal','Count of Crossing','count of Bump','Description','Count of accidents',\n",
    "             'Weather_Condition','Humidity(%)','Precipitation(in)','Wind_Chill(F)','Wind_Speed(mph)']]\n",
    "df.drop = pd.get_dummies(df.drop, columns=['Amenity', \n",
    "    'Bump', \n",
    "    'Crossing',\n",
    "    'Give_Way', \n",
    "    'Junction', \n",
    "    'No_Exit',\n",
    "    'Railway', \n",
    "    'Roundabout', \n",
    "    'Station',\n",
    "    'Stop', \n",
    "    'Traffic_Calming',\n",
    "    'Traffic_Signal', \n",
    "    'Turning_Loop'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86deae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print description table as a plot\n",
    "table = df.drop.describe()\n",
    "\n",
    "dfi.export(table, 'dataframe.png')\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b067079b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visilize count of accident in each state\n",
    "states = df.State.unique()\n",
    "count_by_state=[]\n",
    "for i in df.State.unique():\n",
    "    count_by_state.append(df[df['State']==i].count()['ID'])\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(16,10))\n",
    "sns.barplot(states,count_by_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168f5668",
   "metadata": {},
   "outputs": [],
   "source": [
    "#find top 10 city having accident\n",
    "top_cities=df[\"City\"].value_counts().sort_values()[-20:].reset_index()\n",
    "top_cities.columns=[\"city\",\"number_of_accidents\"]\n",
    "\n",
    "plt.figure(figsize=(10,7))\n",
    "sns.barplot(x=\"city\",y=\"accidents number\",data=top_cities)\n",
    "plt.title(\"TOP 10 CITIES WITH HIGHEST NUMBER OF ACCIDENTS\",fontsize=20)\n",
    "plt.xticks(rotation=40)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b171a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# map represent accident severity \n",
    "\n",
    "severity_cols = {\n",
    "    0: 'green',\n",
    "    1: 'palegreen',\n",
    "    2: 'papayawhip',\n",
    "    3: 'lightsalmon',\n",
    "    4: 'tomato'\n",
    "}\n",
    "\n",
    "vcol = [severity_cols[i] for i in df['Severity']]\n",
    "\n",
    "ax = plt.scatter(df['Start_Lng'], df['Start_Lat'],c = vcol,s=2)\n",
    "plt.title('Accidents representating map by severity level')\n",
    "fig = ax.get_figure()\n",
    "fig.savefig('Severity.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192e951f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# percentage of accident including road params, save as plot\n",
    "road_params = [\n",
    "    'Amenity', \n",
    "    'Bump', \n",
    "    'Crossing',\n",
    "    'Give_Way', \n",
    "    'Junction', \n",
    "    'No_Exit',\n",
    "    'Railway', \n",
    "    'Roundabout', \n",
    "    'Station',\n",
    "    'Stop', \n",
    "    'Traffic_Calming',\n",
    "    'Traffic_Signal', \n",
    "    'Turning_Loop']\n",
    "\n",
    "# % of accident including road params\n",
    "road_param_percent = df.loc[:, road_params].sum() / len(df)\n",
    "plt.title('Presence of road element near accidents')\n",
    "plt.xlabel('% of total of accidents')\n",
    "ax=road_param_percent.sort_values().plot(kind='barh');\n",
    "\n",
    "fig = ax.get_figure()\n",
    "fig.savefig('road.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfeb3315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# percentage of accident by Weather_Condition\n",
    "acc_by_weather_condition = df.groupby('Weather_Condition').size() / len(df)\n",
    "acc_by_weather_condition = acc_by_weather_condition[acc_by_weather_condition > 0.005]\n",
    "plt.title('Presence of weather condition during accidents')\n",
    "plt.xlabel('% of total of accidents')\n",
    "acc_by_weather_condition.sort_values().plot(kind='barh');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c0cb83",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7117bc44",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc04bde9",
   "metadata": {},
   "source": [
    "## kmean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbe9d75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aeafb6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048e2280",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b53dd085",
   "metadata": {},
   "source": [
    "## DBSCAN\n",
    "### New york"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a05e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take lat and lng colunm filter Newyork data\n",
    "NY = df['City'] == 'New York'\n",
    "NY_df = df[NY]\n",
    "NY_loca_df = NY_df[['Start_Lat','Start_Lng']]\n",
    "NY_loca_df.columns = [\"latitude\", \"longitude\"]\n",
    "coords = NY_loca_df[[\"latitude\", \"longitude\"]]\n",
    "X = NY_loca_df.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16262c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#simply plot new york accident coordinate \n",
    "plt.scatter( NY_loca_df[\"longitude\"],NY_loca_df[\"latitude\"],s=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c7f514",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NearestNeighbors find knee point for optimal eps\n",
    "neigh = NearestNeighbors(n_neighbors=5)\n",
    "nbrs = neigh.fit(np.radians(X))\n",
    "distances, indices = nbrs.kneighbors(np.radians(X))\n",
    "distances = distances[:, 1]\n",
    "distances = np.sort(distances, axis=0)\n",
    "fig=plt.figure()\n",
    "plt.plot(distances)\n",
    "plt.xlim(4000, 5570)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe901142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first time try \n",
    "dbscan_cluster_model = DBSCAN(eps=0.000035, min_samples=5, algorithm='ball_tree', metric='haversine').fit(np.radians(X))\n",
    "dbscan_cluster_model\n",
    "dbscan_cluster_model.labels_\n",
    "NY_loca_df['cluster'] = dbscan_cluster_model.labels_\n",
    "location = NY_loca_df['latitude'].mean(), NY_loca_df['longitude'].mean()\n",
    "\n",
    "m = folium.Map(location=location,zoom_start=11,control_scale = True)\n",
    "\n",
    "folium.TileLayer('cartodbpositron').add_to(m)\n",
    "\n",
    "clust_colours = ['#a6cee3','#1f78b4','#b2df8a','#33a02c','#fb9a99','#e31a1c','#fdbf6f','#ff7f00','#cab2d6','#6a3d9a','#ffff99','#b15928']\n",
    "\n",
    "for i in range(0,len(NY_loca_df)):\n",
    "    colouridx = NY_loca_df['cluster'].iloc[i]\n",
    "    if colouridx == -1:\n",
    "        pass\n",
    "    else:\n",
    "        col = clust_colours[colouridx%len(clust_colours)]\n",
    "        folium.CircleMarker([NY_loca_df['latitude'].iloc[i],NY_loca_df['longitude'].iloc[i]], radius = 10, color = col, fill = col).add_to(m)\n",
    "\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae30de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find optimal min_samples based on \n",
    "\n",
    "ss(X, NY_loca_df['cluster'])\n",
    "epsilons = np.linspace(6.75e-05,6.75e-05, num=1)\n",
    "print(epsilons)\n",
    "min_samples = np.arange(2, 100 , step=5) \n",
    "print(min_samples)\n",
    "combinations = list(itertools.product(epsilons, min_samples))\n",
    "print(combinations)\n",
    "N = len(combinations)\n",
    "\n",
    "#define a function to run through all combinations\n",
    "def get_scores_and_labels(combinations, X):\n",
    "  scores = []\n",
    "  all_labels_list = []\n",
    "\n",
    "  for i, (eps, num_samples) in enumerate(combinations):\n",
    "    \n",
    "    dbscan_cluster_model = DBSCAN(eps= eps, min_samples= num_samples, algorithm='ball_tree', metric='haversine').fit(np.radians(X))\n",
    "    labels = dbscan_cluster_model.labels_\n",
    "    labels_set = set(labels)\n",
    "    num_clusters = len(labels_set)\n",
    "    if -1 in labels_set:\n",
    "      num_clusters -= 1\n",
    "    \n",
    "    if (num_clusters < 2) or (num_clusters > 50):\n",
    "      scores.append(-10)\n",
    "      all_labels_list.append('bad')\n",
    "      c = (eps, num_samples)\n",
    "      print(f\"Combination {c} on iteration {i+1} of {N} has {num_clusters} clusters. Moving on\")\n",
    "      continue\n",
    "    \n",
    "    scores.append(ss(X, labels))\n",
    "    all_labels_list.append(labels)\n",
    "    print(f\"Index: {i}, Score: {scores[-1]}, Labels: {all_labels_list[-1]}, NumClusters: {num_clusters}\")\n",
    "\n",
    "  best_index = np.argmax(scores)\n",
    "  best_parameters = combinations[best_index]\n",
    "  best_labels = all_labels_list[best_index]\n",
    "  best_score = scores[best_index]\n",
    "\n",
    "  return {'best_epsilon': best_parameters[0],\n",
    "          'best_min_samples': best_parameters[1], \n",
    "          'best_labels': best_labels,\n",
    "          'best_score': best_score}\n",
    "\n",
    "# find best model\n",
    "best_dict = get_scores_and_labels(combinations, X)\n",
    "NY_loca_df['cluster'] = best_dict['best_labels']\n",
    "best_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a5c128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick clustered data excluding outliers\n",
    "involved = NY_loca_df['cluster'] != -1\n",
    "NY_involved = NY_loca_df[involved]\n",
    "NY_involved\n",
    "\n",
    "Xx = NY_involved[['latitude','longitude']].to_numpy()\n",
    "lablel = NY_involved[['cluster']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940c2d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get clusters centoid \n",
    "num_clusters = len(set(best_dict['best_labels']) - set([-1]))\n",
    "cluster_labels = best_dict['best_labels']\n",
    "clusters = pd.Series([X[cluster_labels == n] for n in range(num_clusters)])\n",
    "clusters\n",
    "\n",
    "#clusters centoid \n",
    "def get_centermost_point(cluster):\n",
    "    centroid = (MultiPoint(cluster).centroid.x, MultiPoint(cluster).centroid.y)\n",
    "    centermost_point = min(cluster, key=lambda point: great_circle(point, centroid).m)\n",
    "    return tuple(centermost_point)\n",
    "\n",
    "# get the centroid point for each cluster\n",
    "centermost_points = clusters.map(get_centermost_point)\n",
    "lats, lons = zip(*centermost_points)\n",
    "rep_points = pd.DataFrame({'lon':lons, 'lat':lats})\n",
    "rep_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae447e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# map all data by folium and save the map\n",
    "location = NY_loca_df['latitude'].mean(), NY_loca_df['longitude'].mean()\n",
    "\n",
    "m = folium.Map(location=location,zoom_start=11,control_scale = True)\n",
    "folium.TileLayer('cartodbpositron').add_to(m)\n",
    "\n",
    "\n",
    "for i in range(0,len(NY_loca_df)):\n",
    "    colouridx = NY_loca_df['cluster'].iloc[i]\n",
    "    if colouridx == -1:\n",
    "        folium.CircleMarker([NY_loca_df['latitude'].iloc[i],NY_loca_df['longitude'].iloc[i]], radius = 5, color = \"white\", fill = \"white\").add_to(m)\n",
    "    else:\n",
    "        col = clust_colours[colouridx%len(clust_colours)]\n",
    "        folium.CircleMarker([NY_loca_df['latitude'].iloc[i],NY_loca_df['longitude'].iloc[i]], radius = 5, color = col, fill = col).add_to(m)\n",
    "        \n",
    "for i in range(len(rep_points)):\n",
    "    folium.CircleMarker([rep_points['lat'].iloc[i],rep_points['lon'].iloc[i]], radius = 2, color = \"black\", fill_opacity=0.7, fill = \"black\").add_to(m)      \n",
    "        \n",
    "m.save(\"ny.html\")\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05595f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# davies_bouldin_score\n",
    "db_index = davies_bouldin_score(X, best_dict['best_labels'])\n",
    "db_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2b3f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ch_index\n",
    "ch_index = calinski_harabasz_score(X, best_dict['best_labels'])\n",
    "print(ch_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ce4019",
   "metadata": {},
   "source": [
    "### US\n",
    "the codes are very same as what we did on NY data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc67c29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take lat and lng colunm \n",
    "US_loca_df = df[['Start_Lat','Start_Lng']]\n",
    "US_loca_df.columns = [\"latitude\", \"longitude\"]\n",
    "coords = US_loca_df[[\"latitude\", \"longitude\"]]\n",
    "X = US_loca_df.to_numpy()\n",
    "\n",
    "# random select 100000 samples from entire US data\n",
    "l = list(range(2243939))\n",
    "random.seed(10)\n",
    "pick = sorted(random.sample(l, 100000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b196a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "##simply plot US accident coordinate \n",
    "plt.scatter( US_picked_df[\"longitude\"],US_picked_df[\"latitude\"],s=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faad1faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#elbow method define range of eps\n",
    "neigh = NearestNeighbors(n_neighbors=300)\n",
    "nbrs = neigh.fit(np.radians(X))\n",
    "distances, indices = nbrs.kneighbors(np.radians(X))\n",
    "distances = distances[:, 1]\n",
    "distances = np.sort(distances, axis=0)\n",
    "fig=plt.figure()\n",
    "plt.plot(distances)\n",
    "plt.xlim(99000, 100300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c531841f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "dbscan_cluster_model = DBSCAN(eps=0.009, min_samples=605, algorithm='ball_tree', metric='haversine').fit(np.radians(X))\n",
    "dbscan_cluster_model\n",
    "dbscan_cluster_model.labels_\n",
    "US_picked_df['cluster'] = dbscan_cluster_model.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54dcb30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# score of test\n",
    "ss(X, US_picked_df['cluster'])\n",
    "# below is testing range of min_s and eps\n",
    "\n",
    "epsilons = np.linspace(0.009,0.011, num=3)\n",
    "min_samples = np.arange(100, 700, step=55) \n",
    "combinations = list(itertools.product(epsilons, min_samples))\n",
    "combinations\n",
    "N = len(combinations)\n",
    "# find best model\n",
    "def get_scores_and_labels(combinations, X):\n",
    "  scores = []\n",
    "  all_labels_list = []\n",
    "  \n",
    "\n",
    "  for i, (eps, num_samples) in enumerate(combinations):\n",
    "    \n",
    "    dbscan_cluster_model = DBSCAN(eps= eps, min_samples= num_samples, algorithm='ball_tree', metric='haversine').fit(np.radians(X))\n",
    "    labels = dbscan_cluster_model.labels_\n",
    "    labels_set = set(labels)\n",
    "    num_clusters = len(labels_set)\n",
    "    if -1 in labels_set:\n",
    "      num_clusters -= 1\n",
    "    \n",
    "    if (num_clusters < 2) or (num_clusters > 50):\n",
    "      scores.append(-10)\n",
    "      all_labels_list.append('bad')\n",
    "      c = (eps, num_samples)\n",
    "      print(f\"Combination {c} on iteration {i+1} of {N} has {num_clusters} clusters. Moving on\")\n",
    "      continue\n",
    "    \n",
    "    scores.append(ss(X, labels))\n",
    "    all_labels_list.append(labels)\n",
    "    print(f\"Index: {i}, Score: {scores[-1]}, Labels: {all_labels_list[-1]}, NumClusters: {num_clusters}\")\n",
    "\n",
    "  best_index = np.argmax(scores)\n",
    "  best_parameters = combinations[best_index]\n",
    "  best_labels = all_labels_list[best_index]\n",
    "  best_score = scores[best_index]\n",
    "\n",
    "  return {'best_epsilon': best_parameters[0],\n",
    "          'best_min_samples': best_parameters[1], \n",
    "          'best_labels': best_labels,\n",
    "          'best_score': best_score}\n",
    "\n",
    "best_dict = get_scores_and_labels(combinations, X)\n",
    "US_picked_df['cluster'] = best_dict['best_labels']\n",
    "best_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477de0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find centroid for each cluster\n",
    "involved = US_picked_df['cluster'] != -1\n",
    "US_involved = US_picked_df[involved]\n",
    "US_involved\n",
    "\n",
    "\n",
    "Xx = US_involved[['latitude','longitude']].to_numpy()\n",
    "lablel = US_involved[['cluster']]\n",
    "num_clusters = len(set(best_dict['best_labels']) - set([-1]))\n",
    "cluster_labels = best_dict['best_labels']\n",
    "clusters = pd.Series([X[cluster_labels == n] for n in range(num_clusters)])\n",
    "clusters\n",
    "\n",
    "def get_centermost_point(cluster):\n",
    "    centroid = (MultiPoint(cluster).centroid.x, MultiPoint(cluster).centroid.y)\n",
    "    centermost_point = min(cluster, key=lambda point: great_circle(point, centroid).m)\n",
    "    return tuple(centermost_point)\n",
    "\n",
    "# get the centroid point for each cluster\n",
    "centermost_points = clusters.map(get_centermost_point)\n",
    "lats, lons = zip(*centermost_points)\n",
    "rep_points = pd.DataFrame({'lon':lons, 'lat':lats})\n",
    "rep_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27536591",
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualisation \n",
    "location = US_picked_df['latitude'].mean(), US_picked_df['longitude'].mean()\n",
    "\n",
    "m = folium.Map(location=location,zoom_start=4,control_scale = True)\n",
    "\n",
    "folium.TileLayer('cartodbpositron').add_to(m)\n",
    "\n",
    "clust_colours = ['#a6cee3','#1f78b4','#b2df8a','#33a02c','#fb9a99','#e31a1c','#fdbf6f','#ff7f00','#cab2d6','#6a3d9a','#ffff99','#b15928']\n",
    "\n",
    "for i in range(0,len(US_picked_df)):\n",
    "    colouridx = US_picked_df['cluster'].iloc[i]\n",
    "    if colouridx != -1:\n",
    "         folium.CircleMarker([US_picked_df['latitude'].iloc[i],US_picked_df['longitude'].iloc[i]], radius = 4, color = \"white\", fill = \"white\").add_to(m)\n",
    "         \n",
    "         col = clust_colours[colouridx%len(clust_colours)]\n",
    "         folium.CircleMarker([US_picked_df['latitude'].iloc[i],US_picked_df['longitude'].iloc[i]], radius = 4, color = col, fill = col).add_to(m)\n",
    "        \n",
    "    else:\n",
    "       \n",
    "        folium.CircleMarker([US_picked_df['latitude'].iloc[i],US_picked_df['longitude'].iloc[i]], radius = 4, color = \"white\", fill = \"white\").add_to(m)\n",
    "for i in range(len(rep_points)):\n",
    "    folium.CircleMarker([rep_points['lat'].iloc[i],rep_points['lon'].iloc[i]], radius = 3, color = \"black\", fill = \"black\").add_to(m)      \n",
    "        \n",
    "m\n",
    "m.save(\"US.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bcc2ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#davies_bouldin_score and calinski_harabasz_score\n",
    "db_index = davies_bouldin_score(X, best_dict['best_labels'])\n",
    "print(db_index)\n",
    "ch_index = calinski_harabasz_score(X, best_dict['best_labels'])\n",
    "print(ch_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326154d4",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb77f7a",
   "metadata": {},
   "source": [
    "# Text Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfbf1439",
   "metadata": {},
   "source": [
    "### Filter out short accident descriptions and common words\n",
    "Only descriptions over 130 characters are kept, and the NLTK stopwords are used to filter out common words from the descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6e66c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = stopwords.words('english')\n",
    "\n",
    "df_longer_info = df[df['Description'].str.len()>130]\n",
    "\n",
    "# ensure all descriptions are of type string\n",
    "df_longer_info['Description'] = df_longer_info['Description'].astype('string')\n",
    "# remove stop words\n",
    "df_longer_info['Description'] = df_longer_info['Description'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0544ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get number of accident, and pairs of accidents for written report\n",
    "n = len(df_longer_info)\n",
    "print(\"# of accident reports: \", n)\n",
    "print(\"# of pairs of accident reports: \", math.comb(n,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20127d3",
   "metadata": {},
   "source": [
    "### Define shingling and hashing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718dfaa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# makes shingles of length 8\n",
    "def shingles(word, n = 8):\n",
    "    return [word[i:i + n] for i in range(len(word) - n + 1)]\n",
    "\n",
    "#hashes a list of strings (from class notes)\n",
    "def listHash(l,seed):\n",
    "\tval = 0\n",
    "\tfor e in l:\n",
    "\t\tval = min(val, mmh3.hash(e, seed))\n",
    "\treturn val "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d210c65",
   "metadata": {},
   "source": [
    "### Shingle the description texts and create a zipped list of accident ID and shingles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40ccf20",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_longer_info['shingles'] = df_longer_info.Description.map(shingles)\n",
    "\n",
    "description_index = list(zip(df_longer_info['ID'], df_longer_info['shingles']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27dd92ea",
   "metadata": {},
   "source": [
    "### Define signatures and Jaccard functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add1088f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def signatures(zipped_dict, seeds):\n",
    "\n",
    "    sig_dict = {k: [] for k in df_longer_info['ID']}\n",
    "\n",
    "    for entry in zipped_dict:\n",
    "        sig_dict[entry[0]] = []\n",
    "\n",
    "        for seed in seeds:\n",
    "            # create a dictionary with accident IDs as index for later reference\n",
    "            sig_dict[entry[0]].append(listHash(entry[1], seed))\n",
    "    \n",
    "    return sig_dict\n",
    "\n",
    "def jaccard(list1, list2):\n",
    "\n",
    "    intersection = len(list(set(list1).intersection(list2)))\n",
    "    union = (len(list1) + len(list2)) - intersection\n",
    "    \n",
    "    return float(intersection) / union"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90eb69f",
   "metadata": {},
   "source": [
    "### Create unique seeds for hashing the signatures \n",
    "A signature dictionary as well as an ordered list are saved for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694f70cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = set()\n",
    "\n",
    "while len(seeds) < 100:\n",
    "    seeds.add(np.random.randint(0,1000))\n",
    "\n",
    "signature_dict = signatures(description_index, seeds)\n",
    "signature_dict_list = list(signature_dict.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb10450",
   "metadata": {},
   "source": [
    "### Create an LSH function \n",
    "This function divides the signatures into b bands and then saves the banded signatures to a dictionary list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3779ba26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lsh(signature_dict_list, b):\n",
    "    n = len(signature_dict_list[0][1])\n",
    "    # ensure that the number of signatures can be evenly divided by the number of bands\n",
    "    assert n % b == 0\n",
    "    r = int(n/b)\n",
    "    band_dict = {} \n",
    "    for i in signature_dict_list:\n",
    "        band_dict[i[0]] = []\n",
    "        for j in range(0, n, r):\n",
    "            #change to append hash\n",
    "            band_dict[i[0]].append(i[1][j:j+r])\n",
    "    return list(band_dict.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68230fb9",
   "metadata": {},
   "source": [
    "### Apply the LSH and search for texts that have bands hashed to the same bucket\n",
    "The loop below will stop comparing two items as soon as one band has been found in the same bucket, these then become a candidate pair (*the code below can take ~5 minutes to run*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2496ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of bands is set to 10, because 10 bands of 10 rows gives a ~0.8 threshold\n",
    "band_dict = lsh(signature_dict_list, 10)\n",
    "\n",
    "result = []\n",
    "\n",
    "for item in list(itertools.combinations(band_dict, 2)):\n",
    "    for sig_band1, sig_band2 in zip(item[0][1], item[1][1]):\n",
    "        if sig_band1==sig_band2:\n",
    "            data = [item[0][0], item[1][0], signature_dict[item[0][0]], signature_dict[item[1][0]]]\n",
    "            result.append(data)\n",
    "            continue\n",
    "\n",
    "df1 = pd.DataFrame(result, columns=['ID1', 'ID2', 'Sigs_1', 'Sigs_2'])\n",
    "\n",
    "df1['similarity'] = df1.apply(lambda x: jaccard(x.Sigs_1, x.Sigs_2), axis=1)\n",
    "\n",
    "# print dataframe with pairs that have over 0.8 similarity but less than 1.0\n",
    "print(df1[(df1['similarity']> 0.8) & (df1['similarity']!=1.0)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567a1afe",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531fa6b0",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28494fb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd3dbc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8067f91f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.11 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "206b81a0a1ee4c6856b9629723b3338ffcec077aadc72468e36aa5b0e1f1cc96"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
