{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d189a067",
   "metadata": {},
   "source": [
    "#                         02807 Computational Tools for Data Science\n",
    "##                                                      Final report \n",
    "\n",
    "group 26\n",
    "Yanan Zhao & s210319\n",
    "Max Specktor & s184362\n",
    "Malthe Dohm Andersen & s194257\n",
    "Kevin Thieu & s221885"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f248b9",
   "metadata": {},
   "source": [
    "**************************************************************************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1484b53d",
   "metadata": {},
   "source": [
    "# Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377f797a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic package\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "# for visual \n",
    "import folium\n",
    "import dataframe_image as dfi\n",
    "# for clustering\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score as ss\n",
    "import itertools\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from matplotlib import pyplot as plt\n",
    "import random\n",
    "from shapely.geometry import MultiPoint\n",
    "from geopy.distance import great_circle\n",
    "from sklearn.metrics import davies_bouldin_score\n",
    "from sklearn.metrics import calinski_harabasz_score\n",
    "import mmh3\n",
    "from nltk.corpus import stopwords\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "\n",
    "#For faster computation\n",
    "from numba import jit \n",
    "\n",
    "#for deep learning:\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.autograd import Variable\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import pickle\n",
    "from matplotlib.pylab import (figure, semilogx, loglog, xlabel, ylabel, legend, \n",
    "                           title, subplot, show, grid)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "import sklearn.linear_model as lm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1fe19ac",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11cd9376",
   "metadata": {},
   "source": [
    "# Data processing \n",
    "Visualise data, pick interesting attributes, and transformed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051ee6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data\n",
    "df = pd.read_csv('../../US_Accidents_May19_Migrated Data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f6f122",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset observing\n",
    "df.shape\n",
    "df.head()\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637b1e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting attribute\n",
    "df.drop = df[['ID','City','State','Severity','Visibility(mi)','Start_Lat','Start_Lng', \n",
    "            'count Traffic Signal','Count of Crossing','count of Bump','Description','Count of accidents',\n",
    "             'Weather_Condition','Humidity(%)','Precipitation(in)','Wind_Chill(F)','Wind_Speed(mph)']]\n",
    "df.drop = pd.get_dummies(df.drop, columns=['Amenity', \n",
    "    'Bump', \n",
    "    'Crossing',\n",
    "    'Give_Way', \n",
    "    'Junction', \n",
    "    'No_Exit',\n",
    "    'Railway', \n",
    "    'Roundabout', \n",
    "    'Station',\n",
    "    'Stop', \n",
    "    'Traffic_Calming',\n",
    "    'Traffic_Signal', \n",
    "    'Turning_Loop'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86deae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print description table as a plot\n",
    "table = df.drop.describe()\n",
    "\n",
    "dfi.export(table, 'dataframe.png')\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b067079b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visilize count of accident in each state\n",
    "states = df.State.unique()\n",
    "count_by_state=[]\n",
    "for i in df.State.unique():\n",
    "    count_by_state.append(df[df['State']==i].count()['ID'])\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(16,10))\n",
    "sns.barplot(states,count_by_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168f5668",
   "metadata": {},
   "outputs": [],
   "source": [
    "#find top 10 city having accident\n",
    "top_cities=df[\"City\"].value_counts().sort_values()[-20:].reset_index()\n",
    "top_cities.columns=[\"city\",\"number_of_accidents\"]\n",
    "\n",
    "plt.figure(figsize=(10,7))\n",
    "sns.barplot(x=\"city\",y=\"accidents number\",data=top_cities)\n",
    "plt.title(\"TOP 10 CITIES WITH HIGHEST NUMBER OF ACCIDENTS\",fontsize=20)\n",
    "plt.xticks(rotation=40)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b171a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# map represent accident severity \n",
    "\n",
    "severity_cols = {\n",
    "    0: 'green',\n",
    "    1: 'palegreen',\n",
    "    2: 'papayawhip',\n",
    "    3: 'lightsalmon',\n",
    "    4: 'tomato'\n",
    "}\n",
    "\n",
    "vcol = [severity_cols[i] for i in df['Severity']]\n",
    "\n",
    "ax = plt.scatter(df['Start_Lng'], df['Start_Lat'],c = vcol,s=2)\n",
    "plt.title('Accidents representating map by severity level')\n",
    "fig = ax.get_figure()\n",
    "fig.savefig('Severity.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192e951f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# percentage of accident including road params, save as plot\n",
    "road_params = [\n",
    "    'Amenity', \n",
    "    'Bump', \n",
    "    'Crossing',\n",
    "    'Give_Way', \n",
    "    'Junction', \n",
    "    'No_Exit',\n",
    "    'Railway', \n",
    "    'Roundabout', \n",
    "    'Station',\n",
    "    'Stop', \n",
    "    'Traffic_Calming',\n",
    "    'Traffic_Signal', \n",
    "    'Turning_Loop']\n",
    "\n",
    "# % of accident including road params\n",
    "road_param_percent = df.loc[:, road_params].sum() / len(df)\n",
    "plt.title('Presence of road element near accidents')\n",
    "plt.xlabel('% of total of accidents')\n",
    "ax=road_param_percent.sort_values().plot(kind='barh');\n",
    "\n",
    "fig = ax.get_figure()\n",
    "fig.savefig('road.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfeb3315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# percentage of accident by Weather_Condition\n",
    "acc_by_weather_condition = df.groupby('Weather_Condition').size() / len(df)\n",
    "acc_by_weather_condition = acc_by_weather_condition[acc_by_weather_condition > 0.005]\n",
    "plt.title('Presence of weather condition during accidents')\n",
    "plt.xlabel('% of total of accidents')\n",
    "acc_by_weather_condition.sort_values().plot(kind='barh');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c0cb83",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7117bc44",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc04bde9",
   "metadata": {},
   "source": [
    "## K-means"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1822a8da",
   "metadata": {},
   "source": [
    "### New york"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbe9d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get data with pd\n",
    "\"\"\"Take in the file\"\"\"\n",
    "df = pd.read_csv('US_Accidents_May19_Migrated Data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aeafb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "city = df.loc[df['City'] == \"New York\"]\n",
    "ny_lat = city[\"Start_Lat\"]\n",
    "ny_lng = city[\"Start_Lng\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048e2280",
   "metadata": {},
   "outputs": [],
   "source": [
    "def comb(lat : list ,long : list) -> list:\n",
    "    data = np.column_stack((lat,long))\n",
    "    return data\n",
    "\n",
    "NY_coord = comb(ny_lat,ny_lng)\n",
    "print(NY_coord[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153833f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(NY_coord[:,0],NY_coord[:,1],s=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53538b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Elbow plot\n",
    "def optimise_k_means(data, max_k): \n",
    "  means = []\n",
    "  inertias = []\n",
    "  \n",
    "  for k in range(1,max_k):\n",
    "      kmeans = KMeans(n_clusters = k) \n",
    "      kmeans.fit(data) \n",
    "      \n",
    "      means.append(k)\n",
    "      inertias.append(kmeans.inertia_)#Look further in to this\n",
    "      \n",
    "  #Generate plot\n",
    "  fig = plt.subplots(figsize = (10,5))\n",
    "  plt.plot(means,inertias, \"o-\")\n",
    "  plt.xlabel(\"Numbers of Clusters\")\n",
    "  plt.ylabel(\"Inertia\")\n",
    "  plt.grid(True)\n",
    "  plt.show()\n",
    "  \n",
    "optimise_k_means(NY_coord,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13709cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tried to make a subplot, but does not work\n",
    "def comparing_n_cluster(max_cluster : int, data):\n",
    "\n",
    "    list_of_means = []\n",
    "    x_pt = np.array([row[0] for row in data])\n",
    "    y_pt = np.array([row[1] for row in data])\n",
    "    for k in range(1,max_cluster):\n",
    "        kmeans = KMeans(n_clusters=k)\n",
    "        kmeans.fit(data)\n",
    "        kmeans_label = kmeans.labels_\n",
    "        list_of_means.append(kmeans_label)\n",
    "    \n",
    "    for i in range(1,max_cluster-1):\n",
    "        plt.scatter(x = x_pt, y = y_pt , c = list_of_means[i])\n",
    "        plt.show()\n",
    "    \n",
    "        \n",
    "comparing_n_cluster(6, NY_coord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a8294b",
   "metadata": {},
   "outputs": [],
   "source": [
    "DBS_score = []\n",
    "for_plot = []\n",
    "for i in range(2,10):\n",
    "    kmeans = KMeans(n_clusters=i, random_state=1).fit(NY_coord)\n",
    "    labels = kmeans.labels_\n",
    "    score = davies_bouldin_score(NY_coord,labels)\n",
    "    DBS_score.append(score)\n",
    "    for_plot.append(i)\n",
    "\n",
    "plt.plot(for_plot,DBS_score,\"o-\")\n",
    "plt.show()\n",
    "\n",
    "print(min(DBS_score))\n",
    "print(np.argmin(DBS_score)+2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f5e75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=3).fit(NY_coord)\n",
    "labels = kmeans.labels_\n",
    "ch_index = calinski_harabasz_score(NY_coord,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2565877f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ch_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9040da5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sil_score = ss(NY_coord,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33eca851",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sil_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c55701",
   "metadata": {},
   "outputs": [],
   "source": [
    "ny_lat = NY_coord[:,0]\n",
    "ny_lng = NY_coord[:,1]\n",
    "location = ny_lat.mean(), ny_lng.mean()\n",
    "m = folium.Map(location=location,zoom_start=11.5,control_scale = True)\n",
    "folium.TileLayer('cartodbpositron').add_to(m)\n",
    "#plugins.MarkerCluster(NY_involved[['latitude','longitude']]).add_to(m)\n",
    "#folium.plugins.HeatMap(NY_involved).add_to(m)\n",
    "\n",
    "kmeans = KMeans(n_clusters = 3)\n",
    "kmeans.fit(NY_coord)\n",
    "kmeans_label = kmeans.labels_\n",
    "\n",
    "\n",
    "clust_colours = ['#a6cee3','#1f78b4','#b2df8a','#33a02c','#fb9a99','#e31a1c','#fdbf6f','#ff7f00','#cab2d6','#6a3d9a','#ffff99','#b15928']\n",
    "\n",
    "for i in range(0,len(NY_coord)):\n",
    "    colouridx = kmeans_label[i]\n",
    "    if colouridx == -1:\n",
    "        folium.CircleMarker([ny_lat[i],ny_lng[i]], radius = 5, color = \"white\", fill = \"white\").add_to(m)\n",
    "    else:\n",
    "        col = clust_colours[colouridx%len(clust_colours)]\n",
    "        folium.CircleMarker([ny_lat[i],ny_lng[i]], radius = 5, color = col, fill = col).add_to(m)\n",
    "        \n",
    "\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1cd6081",
   "metadata": {},
   "source": [
    "## US"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91147c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get data with pd\n",
    "\"\"\"Take in the file\"\"\"\n",
    "df = pd.read_csv('US_Accidents_May19_Migrated Data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3267d8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Acess wanted column and convert to numpy\n",
    "latitude = np.array(df[\"Start_Lat\"])\n",
    "longitude = np.array(df[\"Start_Lng\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81051e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To see if latitude and longitude match in case some data are lost or NaN\n",
    "@jit(nopython = True)\n",
    "def match(lat,lng) -> bool:\n",
    "    if (len(latitude) == len(longitude)):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "print(match(latitude,longitude))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1dc4c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To get easy acess of the data\n",
    "np.savetxt(r'lng.txt', longitude, fmt='%1.6f')\n",
    "np.savetxt(r'lat.txt', latitude, fmt='%1.6f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2ccb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combining the latitude og longitude together\n",
    "@jit \n",
    "def comb(lat : list ,long : list) -> list:\n",
    "    data = np.column_stack((lat,long))\n",
    "    return data\n",
    "\n",
    "data = comb(latitude,longitude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6ab32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Just in case, the coordinates are saved to a file\n",
    "np.savetxt(r'Points.txt', data, fmt='%1.6f') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e8ec57",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choosing random points with seed(10)\n",
    "def random_sample() -> list:\n",
    "    l = list(range(2243939))\n",
    "    random.seed(10)\n",
    "    sorted_list = sorted(random.sample(l, 100000))\n",
    "    return sorted_list\n",
    "\n",
    "pick = np.array(random_sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea9b377",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create an subset from the random sample\n",
    "#@jit\n",
    "def data_sub_set(random_list : list , data_set : list) -> list:\n",
    "    sub_set = np.empty([len(random_list),2])\n",
    "    iterator = 0\n",
    "    for i in random_list:\n",
    "        sub_set[iterator] = data_set[i]\n",
    "        iterator += 1\n",
    "    return sub_set\n",
    "\n",
    "new_set = data_sub_set(pick,data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544ea03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To see how the random data is spread out on a grid\n",
    "new_set = data_sub_set(pick,data)\n",
    "new_lat = np.array([row[0] for row in new_set])\n",
    "new_lng = np.array([row[1] for row in new_set])\n",
    "\n",
    "\n",
    "plt.scatter(new_lat ,new_lng,s=4)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7effdcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To Calculate number of optimal k means\n",
    "#This function shows how many cluster point we should have. \n",
    "def optimise_k_means(data, max_k): \n",
    "  means = []\n",
    "  inertias = []\n",
    "  \n",
    "  for k in range(1,max_k):\n",
    "      kmeans = KMeans(n_clusters = k) \n",
    "      kmeans.fit(data) \n",
    "      \n",
    "      means.append(k)\n",
    "      inertias.append(kmeans.inertia_)\n",
    "      \n",
    "  #Generate plot\n",
    "  fig = plt.subplots(figsize = (10,5))\n",
    "  plt.plot(means,inertias, \"o-\")\n",
    "  plt.xlabel(\"Numbers of Clusters\")\n",
    "  plt.ylabel(\"Inertia\")\n",
    "  plt.grid(True)\n",
    "  plt.show()\n",
    "  \n",
    "optimise_k_means(new_set,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710aae58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate silhoutte_method\n",
    "\n",
    "def silhoutte_method(data, max_k):\n",
    "    sil = []\n",
    "    for k in range(2,max_k+1):\n",
    "        kmeans = KMean(n_clusters = k).fit(data)\n",
    "        labels = kmeans.labels_\n",
    "        score = ss(data,labels)\n",
    "        sil.append(score)\n",
    "    return sil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e71beac",
   "metadata": {},
   "outputs": [],
   "source": [
    "sil_score = silhoutte_method(new_set, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e192c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sil_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae92d488",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calcualte Calinski_harabasz method\n",
    "kmeans = KMeans(n_clusters=3, random_state=1).fit(new_set)\n",
    "labels = kmeans.labels_\n",
    "ch_index = calinski_harabasz_score(new_set,labels )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d68de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ch_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc1ca1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "DBS_score = []\n",
    "for_plot = []\n",
    "for i in range(2,15):\n",
    "    kmeans = KMeans(n_clusters=i, random_state=1).fit(new_set)\n",
    "    labels = kmeans.labels_\n",
    "    score = davies_bouldin_score(new_set, labels)\n",
    "    DBS_score.append(score)\n",
    "    for_plot.append(i)\n",
    "\n",
    "plt.plot(for_plot,DBS_score,\"o-\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa5b847",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get score and value\n",
    "print(min(DBS_score))\n",
    "print(np.argmin(DBS_score)+2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84a4b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualise and compare different size of cluster\n",
    "def comparing_n_cluster(max_cluster : int, data):\n",
    "\n",
    "    list_of_means = []\n",
    "    x_pt = np.array([row[0] for row in new_set])\n",
    "    y_pt = np.array([row[1] for row in data])\n",
    "    for k in range(1,max_cluster):\n",
    "        kmeans = KMeans(n_clusters=k)\n",
    "        kmeans.fit(data)\n",
    "        kmeans_label = kmeans.labels_\n",
    "        list_of_means.append(kmeans_label)\n",
    "    \n",
    "    #fig, axs = plt.subplots(nrows=1, ncols=5, figsize=(20,5))\n",
    "    \n",
    "    #ist_of_means = list(list_of_means)\n",
    "    \n",
    "    #for i, ax in enumerate(fig.axes,start=1):\n",
    "    #    ax.scatter(x = np.array(row[0] for row in data), y = np.array(row[1] for row in data))\n",
    "    for i in range(1,max_cluster-1):\n",
    "        plt.scatter(x = x_pt, y = y_pt , c = list_of_means[i])\n",
    "        plt.show()\n",
    "    \n",
    "        \n",
    "comparing_n_cluster(6, new_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabbcc20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting on a map\n",
    "location = new_lat.mean(), new_lng.mean()\n",
    "\n",
    "m = folium.Map(location=location,zoom_start=11.5,control_scale = True)\n",
    "folium.TileLayer('cartodbpositron').add_to(m)\n",
    "\n",
    "kmeans = KMeans(n_clusters = 3)\n",
    "kmeans.fit(new_set)\n",
    "kmeans_label = kmeans.labels_\n",
    "\n",
    "clust_colours = ['#a6cee3','#1f78b4','#b2df8a','#33a02c','#fb9a99','#e31a1c','#fdbf6f','#ff7f00','#cab2d6','#6a3d9a','#ffff99','#b15928']\n",
    "\n",
    "for i in range(0,len(new_set)):\n",
    "    colouridx = kmeans_label[i]\n",
    "    if colouridx == -1:\n",
    "        folium.CircleMarker([new_lat[i],new_lng[i]], radius = 5, color = \"white\", fill = \"white\").add_to(m)\n",
    "    else:\n",
    "        col = clust_colours[colouridx%len(clust_colours)]\n",
    "        folium.CircleMarker([new_lat[i],new_lng[i]], radius = 5, color = col, fill = col).add_to(m)\n",
    "        \n",
    "\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53dd085",
   "metadata": {},
   "source": [
    "## DBSCAN\n",
    "### New york\n",
    "pick coordinates in NY, map records, use k-distance plot knee point to find optimal eps range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a05e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take lat and lng colunm filter Newyork data\n",
    "NY = df['City'] == 'New York'\n",
    "NY_df = df[NY]\n",
    "NY_loca_df = NY_df[['Start_Lat','Start_Lng']]\n",
    "NY_loca_df.columns = [\"latitude\", \"longitude\"]\n",
    "coords = NY_loca_df[[\"latitude\", \"longitude\"]]\n",
    "X = NY_loca_df.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16262c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#simply plot new york accident coordinate \n",
    "plt.scatter( NY_loca_df[\"longitude\"],NY_loca_df[\"latitude\"],s=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c7f514",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NearestNeighbors find knee point for optimal eps\n",
    "neigh = NearestNeighbors(n_neighbors=5)\n",
    "nbrs = neigh.fit(np.radians(X))\n",
    "distances, indices = nbrs.kneighbors(np.radians(X))\n",
    "distances = distances[:, 1]\n",
    "distances = np.sort(distances, axis=0)\n",
    "fig=plt.figure()\n",
    "plt.plot(distances)\n",
    "plt.xlim(4000, 5570)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe901142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first time try \n",
    "dbscan_cluster_model = DBSCAN(eps=0.000035, min_samples=5, algorithm='ball_tree', metric='haversine').fit(np.radians(X))\n",
    "dbscan_cluster_model\n",
    "dbscan_cluster_model.labels_\n",
    "NY_loca_df['cluster'] = dbscan_cluster_model.labels_\n",
    "location = NY_loca_df['latitude'].mean(), NY_loca_df['longitude'].mean()\n",
    "\n",
    "m = folium.Map(location=location,zoom_start=11,control_scale = True)\n",
    "\n",
    "folium.TileLayer('cartodbpositron').add_to(m)\n",
    "\n",
    "clust_colours = ['#a6cee3','#1f78b4','#b2df8a','#33a02c','#fb9a99','#e31a1c','#fdbf6f','#ff7f00','#cab2d6','#6a3d9a','#ffff99','#b15928']\n",
    "\n",
    "for i in range(0,len(NY_loca_df)):\n",
    "    colouridx = NY_loca_df['cluster'].iloc[i]\n",
    "    if colouridx == -1:\n",
    "        pass\n",
    "    else:\n",
    "        col = clust_colours[colouridx%len(clust_colours)]\n",
    "        folium.CircleMarker([NY_loca_df['latitude'].iloc[i],NY_loca_df['longitude'].iloc[i]], radius = 10, color = col, fill = col).add_to(m)\n",
    "\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af9ca25",
   "metadata": {},
   "source": [
    "With range pf epsilon, We use it for the parameter going forward and try to find the optimal value of MinPts based on Silhouette score. There are several combinations of eps and min_samples to see if we can get best Silhouette score and reasonable clusters. the cose is inspired by https://colab.research.google.com/drive/1DphvjpgQXwBWQq08dMyoSc6UREzXLxSE?usp=sharing#scrollTo=LyXo0mgdOTc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae30de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find optimal min_samples based on \n",
    "\n",
    "ss(X, NY_loca_df['cluster'])\n",
    "epsilons = np.linspace(6.75e-05,8e-05, num=3)\n",
    "print(epsilons)\n",
    "min_samples = np.arange(2, 100 , step=5) \n",
    "print(min_samples)\n",
    "combinations = list(itertools.product(epsilons, min_samples))\n",
    "print(combinations)\n",
    "N = len(combinations)\n",
    "\n",
    "#define a function to run through all combinations\n",
    "def get_scores_and_labels(combinations, X):\n",
    "  scores = []\n",
    "  all_labels_list = []\n",
    "\n",
    "  for i, (eps, num_samples) in enumerate(combinations):\n",
    "    \n",
    "    dbscan_cluster_model = DBSCAN(eps= eps, min_samples= num_samples, algorithm='ball_tree', metric='haversine').fit(np.radians(X))\n",
    "    labels = dbscan_cluster_model.labels_\n",
    "    labels_set = set(labels)\n",
    "    num_clusters = len(labels_set)\n",
    "    if -1 in labels_set:\n",
    "      num_clusters -= 1\n",
    "    \n",
    "    if (num_clusters < 2) or (num_clusters > 50):\n",
    "      scores.append(-10)\n",
    "      all_labels_list.append('bad')\n",
    "      c = (eps, num_samples)\n",
    "      print(f\"Combination {c} on iteration {i+1} of {N} has {num_clusters} clusters. Moving on\")\n",
    "      continue\n",
    "    \n",
    "    scores.append(ss(X, labels))\n",
    "    all_labels_list.append(labels)\n",
    "    print(f\"Index: {i}, Score: {scores[-1]}, Labels: {all_labels_list[-1]}, NumClusters: {num_clusters}\")\n",
    "\n",
    "  best_index = np.argmax(scores)\n",
    "  best_parameters = combinations[best_index]\n",
    "  best_labels = all_labels_list[best_index]\n",
    "  best_score = scores[best_index]\n",
    "\n",
    "  return {'best_epsilon': best_parameters[0],\n",
    "          'best_min_samples': best_parameters[1], \n",
    "          'best_labels': best_labels,\n",
    "          'best_score': best_score}\n",
    "\n",
    "# find best model\n",
    "best_dict = get_scores_and_labels(combinations, X)\n",
    "NY_loca_df['cluster'] = best_dict['best_labels']\n",
    "best_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a5c128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick clustered data excluding outliers\n",
    "involved = NY_loca_df['cluster'] != -1\n",
    "NY_involved = NY_loca_df[involved]\n",
    "NY_involved\n",
    "\n",
    "Xx = NY_involved[['latitude','longitude']].to_numpy()\n",
    "lablel = NY_involved[['cluster']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940c2d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get clusters centoid \n",
    "num_clusters = len(set(best_dict['best_labels']) - set([-1]))\n",
    "cluster_labels = best_dict['best_labels']\n",
    "clusters = pd.Series([X[cluster_labels == n] for n in range(num_clusters)])\n",
    "clusters\n",
    "\n",
    "#clusters centoid \n",
    "def get_centermost_point(cluster):\n",
    "    centroid = (MultiPoint(cluster).centroid.x, MultiPoint(cluster).centroid.y)\n",
    "    centermost_point = min(cluster, key=lambda point: great_circle(point, centroid).m)\n",
    "    return tuple(centermost_point)\n",
    "\n",
    "# get the centroid point for each cluster\n",
    "centermost_points = clusters.map(get_centermost_point)\n",
    "lats, lons = zip(*centermost_points)\n",
    "rep_points = pd.DataFrame({'lon':lons, 'lat':lats})\n",
    "rep_points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3524fd",
   "metadata": {},
   "source": [
    "use folium map all records including noise as white circle, also mark cluster centriods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b49b5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise clusters in map\n",
    "location = NY_loca_df['latitude'].mean(), NY_loca_df['longitude'].mean()\n",
    "\n",
    "m = folium.Map(location=location,zoom_start=11,control_scale = True)\n",
    "folium.TileLayer('cartodbpositron').add_to(m)\n",
    "\n",
    "\n",
    "for i in range(0,len(NY_loca_df)):\n",
    "    colouridx = NY_loca_df['cluster'].iloc[i]\n",
    "    if colouridx == -1:\n",
    "        folium.CircleMarker([NY_loca_df['latitude'].iloc[i],NY_loca_df['longitude'].iloc[i]], radius = 5, color = \"white\", fill = \"white\").add_to(m)\n",
    "    else:\n",
    "        col = clust_colours[colouridx%len(clust_colours)]\n",
    "        folium.CircleMarker([NY_loca_df['latitude'].iloc[i],NY_loca_df['longitude'].iloc[i]], radius = 5, color = col, fill = col).add_to(m)\n",
    "        \n",
    "for i in range(len(rep_points)):\n",
    "    folium.CircleMarker([rep_points['lat'].iloc[i],rep_points['lon'].iloc[i]], radius = 2, color = \"black\", fill_opacity=0.7, fill = \"black\").add_to(m)      \n",
    "        \n",
    "m.save(\"ny.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c102d91",
   "metadata": {},
   "source": [
    "caculate DBscore and CI score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05595f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# davies_bouldin_score\n",
    "db_index = davies_bouldin_score(X, best_dict['best_labels'])\n",
    "db_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2b3f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ch_index\n",
    "ch_index = calinski_harabasz_score(X, best_dict['best_labels'])\n",
    "print(ch_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ce4019",
   "metadata": {},
   "source": [
    "### US\n",
    "the codes are very same as what we did on NY data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc67c29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take lat and lng colunm \n",
    "US_loca_df = df[['Start_Lat','Start_Lng']]\n",
    "US_loca_df.columns = [\"latitude\", \"longitude\"]\n",
    "coords = US_loca_df[[\"latitude\", \"longitude\"]]\n",
    "X = US_loca_df.to_numpy()\n",
    "\n",
    "# random select 100000 samples from entire US data\n",
    "l = list(range(2243939))\n",
    "random.seed(10)\n",
    "pick = sorted(random.sample(l, 100000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b196a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "##simply plot US accident coordinate \n",
    "plt.scatter( US_picked_df[\"longitude\"],US_picked_df[\"latitude\"],s=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faad1faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#elbow method define range of eps\n",
    "neigh = NearestNeighbors(n_neighbors=300)\n",
    "nbrs = neigh.fit(np.radians(X))\n",
    "distances, indices = nbrs.kneighbors(np.radians(X))\n",
    "distances = distances[:, 1]\n",
    "distances = np.sort(distances, axis=0)\n",
    "fig=plt.figure()\n",
    "plt.plot(distances)\n",
    "plt.xlim(99000, 100300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c531841f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "dbscan_cluster_model = DBSCAN(eps=0.009, min_samples=605, algorithm='ball_tree', metric='haversine').fit(np.radians(X))\n",
    "dbscan_cluster_model\n",
    "dbscan_cluster_model.labels_\n",
    "US_picked_df['cluster'] = dbscan_cluster_model.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54dcb30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# score of test\n",
    "ss(X, US_picked_df['cluster'])\n",
    "# below is testing range of min_s and eps\n",
    "\n",
    "epsilons = np.linspace(0.009,0.011, num=3)\n",
    "min_samples = np.arange(100, 700, step=55) \n",
    "combinations = list(itertools.product(epsilons, min_samples))\n",
    "combinations\n",
    "N = len(combinations)\n",
    "# find best model\n",
    "def get_scores_and_labels(combinations, X):\n",
    "  scores = []\n",
    "  all_labels_list = []\n",
    "  \n",
    "\n",
    "  for i, (eps, num_samples) in enumerate(combinations):\n",
    "    \n",
    "    dbscan_cluster_model = DBSCAN(eps= eps, min_samples= num_samples, algorithm='ball_tree', metric='haversine').fit(np.radians(X))\n",
    "    labels = dbscan_cluster_model.labels_\n",
    "    labels_set = set(labels)\n",
    "    num_clusters = len(labels_set)\n",
    "    if -1 in labels_set:\n",
    "      num_clusters -= 1\n",
    "    \n",
    "    if (num_clusters < 2) or (num_clusters > 50):\n",
    "      scores.append(-10)\n",
    "      all_labels_list.append('bad')\n",
    "      c = (eps, num_samples)\n",
    "      print(f\"Combination {c} on iteration {i+1} of {N} has {num_clusters} clusters. Moving on\")\n",
    "      continue\n",
    "    \n",
    "    scores.append(ss(X, labels))\n",
    "    all_labels_list.append(labels)\n",
    "    print(f\"Index: {i}, Score: {scores[-1]}, Labels: {all_labels_list[-1]}, NumClusters: {num_clusters}\")\n",
    "\n",
    "  best_index = np.argmax(scores)\n",
    "  best_parameters = combinations[best_index]\n",
    "  best_labels = all_labels_list[best_index]\n",
    "  best_score = scores[best_index]\n",
    "\n",
    "  return {'best_epsilon': best_parameters[0],\n",
    "          'best_min_samples': best_parameters[1], \n",
    "          'best_labels': best_labels,\n",
    "          'best_score': best_score}\n",
    "\n",
    "best_dict = get_scores_and_labels(combinations, X)\n",
    "US_picked_df['cluster'] = best_dict['best_labels']\n",
    "best_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477de0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find centroid for each cluster\n",
    "involved = US_picked_df['cluster'] != -1\n",
    "US_involved = US_picked_df[involved]\n",
    "US_involved\n",
    "\n",
    "\n",
    "Xx = US_involved[['latitude','longitude']].to_numpy()\n",
    "lablel = US_involved[['cluster']]\n",
    "num_clusters = len(set(best_dict['best_labels']) - set([-1]))\n",
    "cluster_labels = best_dict['best_labels']\n",
    "clusters = pd.Series([X[cluster_labels == n] for n in range(num_clusters)])\n",
    "clusters\n",
    "\n",
    "def get_centermost_point(cluster):\n",
    "    centroid = (MultiPoint(cluster).centroid.x, MultiPoint(cluster).centroid.y)\n",
    "    centermost_point = min(cluster, key=lambda point: great_circle(point, centroid).m)\n",
    "    return tuple(centermost_point)\n",
    "\n",
    "# get the centroid point for each cluster\n",
    "centermost_points = clusters.map(get_centermost_point)\n",
    "lats, lons = zip(*centermost_points)\n",
    "rep_points = pd.DataFrame({'lon':lons, 'lat':lats})\n",
    "rep_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27536591",
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualisation \n",
    "location = US_picked_df['latitude'].mean(), US_picked_df['longitude'].mean()\n",
    "\n",
    "m = folium.Map(location=location,zoom_start=4,control_scale = True)\n",
    "\n",
    "folium.TileLayer('cartodbpositron').add_to(m)\n",
    "\n",
    "clust_colours = ['#a6cee3','#1f78b4','#b2df8a','#33a02c','#fb9a99','#e31a1c','#fdbf6f','#ff7f00','#cab2d6','#6a3d9a','#ffff99','#b15928']\n",
    "\n",
    "for i in range(0,len(US_picked_df)):\n",
    "    colouridx = US_picked_df['cluster'].iloc[i]\n",
    "    if colouridx != -1:\n",
    "         folium.CircleMarker([US_picked_df['latitude'].iloc[i],US_picked_df['longitude'].iloc[i]], radius = 4, color = \"white\", fill = \"white\").add_to(m)\n",
    "         \n",
    "         col = clust_colours[colouridx%len(clust_colours)]\n",
    "         folium.CircleMarker([US_picked_df['latitude'].iloc[i],US_picked_df['longitude'].iloc[i]], radius = 4, color = col, fill = col).add_to(m)\n",
    "        \n",
    "    else:\n",
    "       \n",
    "        folium.CircleMarker([US_picked_df['latitude'].iloc[i],US_picked_df['longitude'].iloc[i]], radius = 4, color = \"white\", fill = \"white\").add_to(m)\n",
    "for i in range(len(rep_points)):\n",
    "    folium.CircleMarker([rep_points['lat'].iloc[i],rep_points['lon'].iloc[i]], radius = 3, color = \"black\", fill = \"black\").add_to(m)      \n",
    "        \n",
    "m\n",
    "m.save(\"US.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bcc2ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#davies_bouldin_score and calinski_harabasz_score\n",
    "db_index = davies_bouldin_score(X, best_dict['best_labels'])\n",
    "print(db_index)\n",
    "ch_index = calinski_harabasz_score(X, best_dict['best_labels'])\n",
    "print(ch_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326154d4",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb77f7a",
   "metadata": {},
   "source": [
    "# Text Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfbf1439",
   "metadata": {},
   "source": [
    "### Filter out short accident descriptions and common words\n",
    "Only descriptions over 130 characters are kept, and the NLTK stopwords are used to filter out common words from the descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6e66c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = stopwords.words('english')\n",
    "\n",
    "df_longer_info = df[df['Description'].str.len()>130]\n",
    "\n",
    "# ensure all descriptions are of type string\n",
    "df_longer_info['Description'] = df_longer_info['Description'].astype('string')\n",
    "# remove stop words\n",
    "df_longer_info['Description'] = df_longer_info['Description'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0544ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get number of accident, and pairs of accidents for written report\n",
    "n = len(df_longer_info)\n",
    "print(\"# of accident reports: \", n)\n",
    "print(\"# of pairs of accident reports: \", math.comb(n,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20127d3",
   "metadata": {},
   "source": [
    "### Define shingling and hashing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718dfaa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# makes shingles of length 8\n",
    "def shingles(word, n = 8):\n",
    "    return [word[i:i + n] for i in range(len(word) - n + 1)]\n",
    "\n",
    "#hashes a list of strings (from class notes)\n",
    "def listHash(l,seed):\n",
    "\tval = 0\n",
    "\tfor e in l:\n",
    "\t\tval = min(val, mmh3.hash(e, seed))\n",
    "\treturn val "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d210c65",
   "metadata": {},
   "source": [
    "### Shingle the description texts and create a zipped list of accident ID and shingles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40ccf20",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_longer_info['shingles'] = df_longer_info.Description.map(shingles)\n",
    "\n",
    "description_index = list(zip(df_longer_info['ID'], df_longer_info['shingles']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27dd92ea",
   "metadata": {},
   "source": [
    "### Define signatures and Jaccard functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add1088f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def signatures(zipped_dict, seeds):\n",
    "\n",
    "    sig_dict = {k: [] for k in df_longer_info['ID']}\n",
    "\n",
    "    for entry in zipped_dict:\n",
    "        sig_dict[entry[0]] = []\n",
    "\n",
    "        for seed in seeds:\n",
    "            # create a dictionary with accident IDs as index for later reference\n",
    "            sig_dict[entry[0]].append(listHash(entry[1], seed))\n",
    "    \n",
    "    return sig_dict\n",
    "\n",
    "def jaccard(list1, list2):\n",
    "\n",
    "    intersection = len(list(set(list1).intersection(list2)))\n",
    "    union = (len(list1) + len(list2)) - intersection\n",
    "    \n",
    "    return float(intersection) / union"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90eb69f",
   "metadata": {},
   "source": [
    "### Create unique seeds for hashing the signatures \n",
    "A signature dictionary as well as an ordered list are saved for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694f70cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = set()\n",
    "\n",
    "while len(seeds) < 100:\n",
    "    seeds.add(np.random.randint(0,1000))\n",
    "\n",
    "signature_dict = signatures(description_index, seeds)\n",
    "signature_dict_list = list(signature_dict.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb10450",
   "metadata": {},
   "source": [
    "### Create an LSH function \n",
    "This function divides the signatures into b bands and then saves the banded signatures to a dictionary list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3779ba26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lsh(signature_dict_list, b):\n",
    "    n = len(signature_dict_list[0][1])\n",
    "    # ensure that the number of signatures can be evenly divided by the number of bands\n",
    "    assert n % b == 0\n",
    "    r = int(n/b)\n",
    "    band_dict = {} \n",
    "    for i in signature_dict_list:\n",
    "        band_dict[i[0]] = []\n",
    "        for j in range(0, n, r):\n",
    "            #change to append hash\n",
    "            band_dict[i[0]].append(i[1][j:j+r])\n",
    "    return list(band_dict.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68230fb9",
   "metadata": {},
   "source": [
    "### Apply the LSH and search for texts that have bands hashed to the same bucket\n",
    "The loop below will stop comparing two items as soon as one band has been found in the same bucket, these then become a candidate pair (*the code below can take ~5 minutes to run*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2496ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of bands is set to 10, because 10 bands of 10 rows gives a ~0.8 threshold\n",
    "band_dict = lsh(signature_dict_list, 10)\n",
    "\n",
    "result = []\n",
    "\n",
    "for item in list(itertools.combinations(band_dict, 2)):\n",
    "    for sig_band1, sig_band2 in zip(item[0][1], item[1][1]):\n",
    "        if sig_band1==sig_band2:\n",
    "            data = [item[0][0], item[1][0], signature_dict[item[0][0]], signature_dict[item[1][0]]]\n",
    "            result.append(data)\n",
    "            continue\n",
    "\n",
    "df1 = pd.DataFrame(result, columns=['ID1', 'ID2', 'Sigs_1', 'Sigs_2'])\n",
    "\n",
    "df1['similarity'] = df1.apply(lambda x: jaccard(x.Sigs_1, x.Sigs_2), axis=1)\n",
    "\n",
    "# print dataframe with pairs that have over 0.8 similarity but less than 1.0\n",
    "print(df1[(df1['similarity']> 0.8) & (df1['similarity']!=1.0)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567a1afe",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531fa6b0",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54468380",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data\n",
    "df = pd.read_csv('US_accidents_May19_Migrated Data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc24ca1d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Get a look at the data we're working with\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f833d4f",
   "metadata": {},
   "source": [
    "We make a histogram of the severity column, and see that the values 2,3,4 are massively overrepresented as compared to values 0 and 1. 1 is still somewhat represented, but there are almost no datapoints with severity 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49468015",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df['Severity'],bins=5)\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f5b465",
   "metadata": {},
   "outputs": [],
   "source": [
    "#See what features we have in the datset\n",
    "df.columns.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b4bc49",
   "metadata": {},
   "source": [
    "Time should be an important feature for predicting the severity of an accident. One would expect that more serious accidents happen at night and during the winter for example. We can encode the time into a few different features: the year of the accident, the month, the hour of the day, and the weekday. Additionally, from the start_time and End_Time features, we can calculate how long the accident stop the flow of traffic, as a long traffic stoppage likely means that the accident was more severe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbdfb886",
   "metadata": {},
   "outputs": [],
   "source": [
    "#duration feature using start and end time\n",
    "df['Start_Time'] = pd.to_datetime(df['Start_Time'], errors='coerce')\n",
    "df['End_Time'] = pd.to_datetime(df['End_Time'], errors='coerce')\n",
    "\n",
    "# Extract year, month, day, hour and weekday\n",
    "df['Year']=df['Start_Time'].dt.year\n",
    "df['Month']=df['Start_Time'].dt.month\n",
    "\n",
    "df['Hour']=df['Start_Time'].dt.hour\n",
    "df['Weekday']=df['Start_Time'].dt.strftime('%a')\n",
    "# Extract the amount of time in the unit of minutes for each accident, round to the nearest integer\n",
    "df['Duration']=round((df['End_Time']-df['Start_Time'])/np.timedelta64(1,'m'))\n",
    "\n",
    "#adapted from https://medium.com/@vaibhavgope02/predicting-accident-severity-with-us-accidents-dataset-4aeaaae0b0af"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf3481f",
   "metadata": {},
   "outputs": [],
   "source": [
    "set(df['Weekday'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13be253d",
   "metadata": {},
   "source": [
    "Select which features we would like to keep:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298d4fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Good_columns = ['Year','Month','Hour','Weekday','Duration', 'count of Bump','Count of Crossing','count Traffic Signal','Crossing','Junction','No_Exit','Railway','Roundabout','Side','Station','Stop','Sunrise_Sunset','Traffic_Calming','Traffic_Signal','Temperature(F)','Visibility(mi)','Weather_Condition','Humidity(%)','Precipitation(in)','Pressure(in)','Wind_Speed(mph)','Start_Lat','Start_Lng','Severity']\n",
    "#List of all used columns:\n",
    "#the features listed below contains strings or true/false values that needs to be transformed:\n",
    "to_be_transformed = ['Crossing','Junction','No_Exit','Railway','Roundabout','Side','Station','Stop','Sunrise_Sunset','Traffic_Calming','Traffic_Signal','Weather_Condition','Weekday']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72fc4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df=df[Good_columns]\n",
    "#fill 0-values in the Precipitation feature, since we assume that the lack of data means that there was no rainfall.\n",
    "new_df['Precipitation(in)']=new_df['Precipitation(in)'].fillna(value=0)\n",
    "new_df = new_df.dropna(axis=0,how='any').reset_index()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e9e9b3",
   "metadata": {},
   "source": [
    "Weather conditions has many different values, so we one hot encode it, since one-hot encoding usually works well for learning categorical features with deep learning models since the model can intepret higher numbers in ordinal features as more important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbd00b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#One-hot encoding for the Weather_Condition parameter\n",
    "y = pd.get_dummies(new_df.Weather_Condition, prefix='Weather_Condition')\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f378e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.concat([new_df,y],axis=1)\n",
    "new_df = new_df.drop('Weather_Condition',axis=1)\n",
    "new_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277bfe5c",
   "metadata": {},
   "source": [
    "We could also want to one-hot encode Years, however the result in the report was achieved without one-hot encoded Years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4dfbde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#One hot encode the Year feature we made earlier\n",
    "y = pd.get_dummies(new_df.Year, prefix='Year')\n",
    "y\n",
    "new_df = pd.concat([new_df,y],axis=1)\n",
    "new_df = new_df.drop('Year',axis=1)\n",
    "new_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8758e6",
   "metadata": {},
   "source": [
    "We can also one-hot encode \"Weekday\". This was done rather than the cyclical transformation presented later for the model shown in the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c8bfde",
   "metadata": {},
   "outputs": [],
   "source": [
    "#One hot encode the Weekday feature we made earlier\n",
    "y = pd.get_dummies(new_df.Weekday, prefix='Weekday')\n",
    "y\n",
    "new_df = pd.concat([new_df,y],axis=1)\n",
    "new_df = new_df.drop('Weekday',axis=1)\n",
    "new_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e9a1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for NaN-values:\n",
    "new_df.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919b37db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#change \"true/false\" features to binary, ordinal features, and categorical features to ordinal features.\n",
    "\n",
    "#\n",
    "to_be_transformed = []\n",
    "for column in tqdm(new_df.columns):\n",
    "    if type(new_df[column][0])==type('string'):\n",
    "        to_be_transformed.append(column)\n",
    "    elif new_df[column][0]==True or new_df[column][0]==False: #replace \"true\" with 1 and \"false\" with 0\n",
    "        new_df[column]=new_df[column].astype(int)\n",
    "#print which features still need to be transformed\n",
    "print(to_be_transformed)\n",
    "transform_dic = {}\n",
    "for column in tqdm(to_be_transformed): #loop over the remaining columns\n",
    "    dic = {}\n",
    "    count = 0\n",
    "    for item in new_df[column]: #loop over each element in the column\n",
    "        if item == True:\n",
    "            dic[item]=1\n",
    "        elif item == False:\n",
    "            dic[item]=0\n",
    "        elif item not in dic:\n",
    "            dic[item]=count\n",
    "            count+=1\n",
    "    transform_dic[column]=dic\n",
    "#Now that we have the new values for the transformed columns we assign the new values\n",
    "for column in tqdm(to_be_transformed):\n",
    "    new_list = []\n",
    "    for i in range(len(new_df[column])):\n",
    "        new_list.append(transform_dic[column][new_df[column][i]])\n",
    "    new_df[column]=new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d843f50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "backup_df=new_df.copy()#IGNORE THIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a69cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since there are almost 0 cases where the severity is 0, we remove the points that have the value. This will likely improve performance.\n",
    "new_df.drop(new_df[new_df.Severity==0].index,inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87555fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sanity check, check that transformation worked\n",
    "set(new_df['Weekday'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a39e00b",
   "metadata": {},
   "source": [
    "For the time features, we can use cyclical encoding. Cyclical encoding maps the features to sine and cosine. It is useful for time features, since hour 24 is essentially the same as hour 0. Based on the following article:https://www.kaggle.com/code/avanwyk/encoding-cyclical-features-for-deep-learning\n",
    "This was not done in the run shown in the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a42c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df['Hour_sin'] = np.sin(2*np.pi*new_df['Hour']/24.0)\n",
    "new_df['Hour_cos'] = np.cos(2*np.pi*new_df['Hour']/24.0)\n",
    "new_df['Month_sin'] = np.sin(2*np.pi*new_df['Month']/12)\n",
    "new_df['Month_cos'] = np.cos(2*np.pi*new_df['Month']/12)\n",
    "new_df['Weekday_sin'] = np.cos(2*np.pi*new_df['Weekday']/7)\n",
    "new_df['Weekday_cos'] = np.cos(2*np.pi*new_df['Weekday']/7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5ae72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.drop(['index','Hour','Month','Weekday'],axis=1,inplace=True)#drop old time columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f620ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save finished dataframe as pickle. If running training code on google colab, upload this to google drive.\n",
    "with open('final_df.pickle', 'wb') as handle:\n",
    "    pickle.dump(new_df, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continent-pursuit",
   "metadata": {},
   "source": [
    "### Feed Forward Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5kLnsFNDelI_",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5kLnsFNDelI_",
    "outputId": "28c8887f-036e-421d-df6d-5831391b7b7c"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive #For google colab integration\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ZQ79KrtAZii",
   "metadata": {
    "id": "7ZQ79KrtAZii"
   },
   "source": [
    "First we load the data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yHl0LeLyemGY",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yHl0LeLyemGY",
    "outputId": "270295b1-7b2c-4b05-e126-80b99909aebd"
   },
   "outputs": [],
   "source": [
    "!pip3 install pickle5\n",
    "import pickle5 as pickle\n",
    "# df = pd.read_csv('/content/drive/MyDrive/Computational Tools/final_df234.csv')\n",
    "with open('/content/drive/MyDrive/Computational_Tools/final_df234.pickle', 'rb') as f:\n",
    "  df = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yO95LObl7i5e",
   "metadata": {
    "id": "yO95LObl7i5e"
   },
   "outputs": [],
   "source": [
    "n = 1500000 #Sample size\n",
    "df_subset = df.sample(n).copy()\n",
    "#split into data and targets:\n",
    "X = df_subset.drop(columns = ['Severity']).copy() \n",
    "y = df_subset['Severity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e24e428",
   "metadata": {
    "id": "6e24e428"
   },
   "outputs": [],
   "source": [
    "#Normalize data\n",
    "normalized_df=(X-X.mean())/X.std()\n",
    "\n",
    "#Split data into training(95%), validation(2.5%) and test split(2.5%):\n",
    "X_train, X_rem, y_train, y_rem = train_test_split(X,y,train_size = 0.95)\n",
    "\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_rem,y_rem, test_size=0.5)\n",
    "\n",
    "#the data set contains far more datapoints of severity 2 then the other values, making severity 2 the most common category. \n",
    "#To avoid this messing with the outcome, we try undersampling classes Severity 2 and 3, and oversampling category 1.\n",
    "train_data = pd.concat([X_train, y_train], axis=1)\n",
    "\n",
    "#Separate the dataset by class\n",
    "Sev1 = train_data[train_data.Severity==1]\n",
    "Sev2 = train_data[train_data.Severity==2]\n",
    "Sev3 = train_data[train_data.Severity==3]\n",
    "Sev4 = train_data[train_data.Severity==4]\n",
    "Majority_list = [Sev1,Sev2,Sev3]\n",
    "#Set over and undersampling ratios\n",
    "# over_under_size = [int(a*(len(Sev4)/50000)) for a in [10000,100000,70000]]\n",
    "over_under_size = [len(Sev4),len(Sev4),len(Sev4)]\n",
    "downsampled = Sev4\n",
    "for i in range(len(Majority_list)):\n",
    "  Maj_downsampled = resample(Majority_list[i], replace=True,  n_samples=over_under_size[i], random_state=27) #resample data\n",
    "  downsampled = pd.concat([downsampled,Maj_downsampled],axis=0)\n",
    "#Set data to tensors of correct datatype:\n",
    "X_train = torch.Tensor(downsampled.drop(columns = [\"Severity\"]).copy().values).type(torch.float64)\n",
    "y_train = torch.Tensor(downsampled['Severity'].values).type(torch.LongTensor)-1 #We subtract one so the smallest class is 0.\n",
    "X_test = torch.Tensor(X_test.values).type(torch.float64)\n",
    "X_val = torch.Tensor(X_val.values).type(torch.float64)\n",
    "y_test = torch.Tensor(y_test.values)-1\n",
    "y_val = torch.Tensor(y_val.values)-1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "N0XLC3L9hmoI",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N0XLC3L9hmoI",
    "outputId": "97994764-b17a-4d1f-c804-be7e56960e85"
   },
   "outputs": [],
   "source": [
    "#Check the size of the training and validation set:\n",
    "print(len(X_train),len(y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "QdxGzR20q0jI",
   "metadata": {
    "id": "QdxGzR20q0jI"
   },
   "source": [
    "We can check if the data can predict the class label with a simple linear regression. We do this to get an idea of how difficult the problem is to solve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Og3f8cHWfBHc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 302
    },
    "id": "Og3f8cHWfBHc",
    "outputId": "ef7ae6a6-e7a3-4318-a0c2-335a90f47802"
   },
   "outputs": [],
   "source": [
    "#We use an sklearn implementation of linear regression since we only want to check if the problem is easily solvable:\n",
    "model = lm.LinearRegression()\n",
    "model.fit(X_train,y_train)\n",
    "\n",
    "# Predict alcohol content\n",
    "y_est = model.predict(X_test)\n",
    "residual = y_est-y_test.numpy()\n",
    "\n",
    "# Display scatter plot\n",
    "figure()\n",
    "subplot(2,1,1)\n",
    "plt.plot(y_test, y_est, '.')\n",
    "plt.xlabel('Severity (true)'); plt.ylabel('Severity (estimated)');\n",
    "  \n",
    "subplot(2,1,2)\n",
    "plt.hist(residual,bins=[0,0.5,1.5,2.5,3.5,4])\n",
    "\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yAHHO_iirG2B",
   "metadata": {
    "id": "yAHHO_iirG2B"
   },
   "source": [
    "The problem is not easily solved, since the linear regression cannot distinguish between the classes\n",
    "\n",
    "Below we build the network(adapted from 02456 exercise 3.3 2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "D6avV4chZsM2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D6avV4chZsM2",
    "outputId": "4b3ec50d-d89c-4d54-c525-816f5cf9d37e"
   },
   "outputs": [],
   "source": [
    "#Hyperparameters\n",
    "num_output = 4\n",
    "num_l1 = 1024\n",
    "num_l2 = 2048\n",
    "num_features = X_train.shape[1]\n",
    "\n",
    "# define network\n",
    "class Net(nn.Module):\n",
    "  def __init__(self,num_features, num_hidden, num_hidden_2, num_output):\n",
    "    super(Net, self).__init__()\n",
    "    self.linear1 = nn.Linear(num_features,num_l1)\n",
    "    self.linear2 = nn.Linear(num_l1,num_l2)\n",
    "    self.linear3 = nn.Linear(num_l2,num_l1)\n",
    "    self.linear4 = nn.Linear(num_l1,num_output)\n",
    "    # self.linear3 = nn.Linear(num_l2,num_output)\n",
    "\n",
    "\n",
    "    # self.activation = torch.nn.Tanh()\n",
    "    self.activation = torch.nn.ReLU()\n",
    "    self.dropout = torch.nn.Dropout(p=0.6)\n",
    "    self.batchnorm1 = torch.nn.BatchNorm1d(num_hidden)\n",
    "    self.batchnorm2 = torch.nn.BatchNorm1d(num_hidden_2)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.linear1(x)\n",
    "    x = self.dropout(x)\n",
    "    x = self.activation(x)\n",
    "    x = self.batchnorm1(x)\n",
    "    x = self.linear2(x)\n",
    "    x = self.dropout(x)\n",
    "    x = self.activation(x)\n",
    "    x = self.batchnorm2(x)\n",
    "    x = self.linear3(x)\n",
    "    x = self.dropout(x)\n",
    "    x = self.batchnorm1(x)\n",
    "    x = self.linear4(x)\n",
    "    return F.softmax(x,dim=1)\n",
    "\n",
    "net = Net(num_features, num_l1, num_l2, num_output).double()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print('##converting network to cuda-enabled')\n",
    "    net.cuda()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e513161f",
   "metadata": {
    "id": "e513161f"
   },
   "outputs": [],
   "source": [
    "#Define the optimizer and criterion(Loss function)\n",
    "optimizer = optim.Adam(net.parameters(), lr=1e-4, weight_decay = 1e-5)\n",
    "criterion = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "utrDsyz8qgf9",
   "metadata": {
    "id": "utrDsyz8qgf9"
   },
   "source": [
    "Below we have the training loop for the network (adapted from 02456 exercise 3.3)\n",
    "In order to run this code, cuda will have to be available on your machine. Otherwise it can run on google colab using a GPU runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed87fd5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 287
    },
    "id": "1ed87fd5",
    "outputId": "db0f803b-ee9c-4e39-c1c1-7a4b68f362d6"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# setting hyperparameters and gettings epoch sizes\n",
    "batch_size = 4096\n",
    "num_epochs = 100\n",
    "num_samples_train = X_train.shape[0]\n",
    "num_batches_train = num_samples_train // batch_size\n",
    "num_samples_valid = X_val.shape[0]\n",
    "num_batches_valid = num_samples_valid // batch_size\n",
    "\n",
    "# setting up lists for handling loss/accuracy\n",
    "train_acc, train_loss = [], []\n",
    "valid_acc, valid_loss = [], []\n",
    "test_acc, test_loss = [], []\n",
    "cur_loss = 0\n",
    "losses = []\n",
    "\n",
    "get_slice = lambda i, size: range(i * size, (i + 1) * size)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward -> Backprob -> Update params\n",
    "    ## Train\n",
    "    cur_loss = 0\n",
    "    net.train()\n",
    "    for i in range(num_batches_train):\n",
    "\n",
    "\n",
    "        slce = get_slice(i, batch_size)\n",
    "        input = X_train[slce].cuda()\n",
    "        # input = input.cuda()\n",
    "        output = net(input)\n",
    "        \n",
    "        # compute gradients given loss\n",
    "        target_batch = y_train[slce]\n",
    "        target_batch = target_batch.cuda()\n",
    "        batch_loss = criterion(output, target_batch)\n",
    "        # batch_loss.requires_grad=True\n",
    "        optimizer.zero_grad()\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        cur_loss += batch_loss  \n",
    "    losses.append(cur_loss / batch_size)\n",
    "\n",
    "    net.eval()\n",
    "    ### Evaluate training\n",
    "    train_preds, train_targs = [], []\n",
    "    for i in range(num_batches_train):\n",
    "        slce = get_slice(i, batch_size)\n",
    "        input = X_train[slce].cuda()\n",
    "        output = net(input)\n",
    "        \n",
    "        preds = torch.max(output, 1)[1]\n",
    "        \n",
    "        train_targs += list(y_train[slce].cpu().numpy())\n",
    "        train_preds += list(preds.data.cpu().numpy())\n",
    "    \n",
    "    ### Evaluate validation\n",
    "    val_preds, val_targs = [], []\n",
    "    for i in range(num_batches_valid):\n",
    "        slce = get_slice(i, batch_size)\n",
    "        input = X_val[slce].cuda()\n",
    "        output = net(input)\n",
    "        preds = torch.max(output, 1)[1]\n",
    "        val_targs += list(y_val[slce].cpu().numpy())\n",
    "        val_preds += list(preds.data.cpu().numpy())\n",
    "        \n",
    "\n",
    "    train_acc_cur = accuracy_score(train_targs, train_preds)\n",
    "    valid_acc_cur = accuracy_score(val_targs, val_preds)\n",
    "    \n",
    "    train_acc.append(train_acc_cur)\n",
    "    valid_acc.append(valid_acc_cur)\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(\"Epoch %2i : Train Loss %f , Train acc %f, Valid acc %f\" % (\n",
    "                epoch+1, losses[-1], train_acc_cur, valid_acc_cur))\n",
    "\n",
    "epoch = np.arange(len(train_acc))\n",
    "plt.figure()\n",
    "plt.plot(epoch, train_acc, 'r', epoch, valid_acc, 'b')\n",
    "plt.legend(['Train Accucary','Validation Accuracy'])\n",
    "plt.xlabel('Updates'), plt.ylabel('Acc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Z84eR-qfrVgK",
   "metadata": {
    "id": "Z84eR-qfrVgK"
   },
   "source": [
    "Please not that due to the randomness involved in sampling data points, splitting the dataset and initializing parameters, results of training may not be exactly the same as reported in the paper.\n",
    "\n",
    "Below we plot the loss curve for the training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vMFIFywmNvj8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "id": "vMFIFywmNvj8",
    "outputId": "b9144e4c-5ea9-42a6-afcf-6e7a7a206098"
   },
   "outputs": [],
   "source": [
    "losses = [loss.cpu().detach().numpy() for loss in losses]\n",
    "plt.figure()\n",
    "plt.plot(epoch, losses)\n",
    "plt.legend(['Loss'])\n",
    "plt.xlabel('Updates') #, plt.ylabel('')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uZQe2KJ-rawL",
   "metadata": {
    "id": "uZQe2KJ-rawL"
   },
   "source": [
    "Then we can plot a confusion matrix to tell how our model performed, and in what mistakes the network made, if any."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2weTbxiSrjS5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 461
    },
    "id": "2weTbxiSrjS5",
    "outputId": "b3be9525-eac4-440c-a816-e74a51b3691f"
   },
   "outputs": [],
   "source": [
    "net.cpu()\n",
    "preds = torch.max(net(X_test), 1)[1]\n",
    "cm = confusion_matrix(y_test,preds)\n",
    "df_cm = pd.DataFrame(cm, index = range(1,5), columns = range(1,5))\n",
    "plt.figure(figsize=(10,7))\n",
    "sn.heatmap(df_cm, annot=True)\n",
    "plt.title('Test Accuracy: {}'.format(accuracy_score(y_test,preds)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "cda9ce8d37873781ffa156bce1ba1ba8d7b444f84fa8e094327ec1cc704afe1d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
