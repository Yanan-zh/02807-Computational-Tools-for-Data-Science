{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d189a067",
   "metadata": {},
   "source": [
    "# install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377f797a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import folium\n",
    "import dataframe_image as dfi\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics import silhouette_score as ss\n",
    "import itertools\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from matplotlib import pyplot as plt\n",
    "import random\n",
    "from shapely.geometry import MultiPoint\n",
    "from geopy.distance import great_circle\n",
    "from sklearn.metrics import davies_bouldin_score\n",
    "from sklearn.metrics import calinski_harabasz_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1fe19ac",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11cd9376",
   "metadata": {},
   "source": [
    "# Data processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051ee6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data\n",
    "df = pd.read_csv('../../US_Accidents_May19_Migrated Data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f6f122",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset observing\n",
    "df.shape\n",
    "df.head()\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637b1e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting attribute\n",
    "df.drop = df[['ID','City','State','Severity','Visibility(mi)','Start_Lat','Start_Lng', \n",
    "            'count Traffic Signal','Count of Crossing','count of Bump','Description','Count of accidents',\n",
    "             'Weather_Condition','Humidity(%)','Precipitation(in)','Wind_Chill(F)','Wind_Speed(mph)']]\n",
    "df.drop = pd.get_dummies(df.drop, columns=['Amenity', \n",
    "    'Bump', \n",
    "    'Crossing',\n",
    "    'Give_Way', \n",
    "    'Junction', \n",
    "    'No_Exit',\n",
    "    'Railway', \n",
    "    'Roundabout', \n",
    "    'Station',\n",
    "    'Stop', \n",
    "    'Traffic_Calming',\n",
    "    'Traffic_Signal', \n",
    "    'Turning_Loop'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86deae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print description table as a plot\n",
    "table = df.drop.describe()\n",
    "\n",
    "dfi.export(table, 'dataframe.png')\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b067079b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visilize count of accident in each state\n",
    "states = df.State.unique()\n",
    "count_by_state=[]\n",
    "for i in df.State.unique():\n",
    "    count_by_state.append(df[df['State']==i].count()['ID'])\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(16,10))\n",
    "sns.barplot(states,count_by_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168f5668",
   "metadata": {},
   "outputs": [],
   "source": [
    "#find top 10 city having accident\n",
    "top_cities=df[\"City\"].value_counts().sort_values()[-20:].reset_index()\n",
    "top_cities.columns=[\"city\",\"number_of_accidents\"]\n",
    "\n",
    "plt.figure(figsize=(10,7))\n",
    "sns.barplot(x=\"city\",y=\"accidents number\",data=top_cities)\n",
    "plt.title(\"TOP 10 CITIES WITH HIGHEST NUMBER OF ACCIDENTS\",fontsize=20)\n",
    "plt.xticks(rotation=40)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b171a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# map represent accident severity \n",
    "\n",
    "severity_cols = {\n",
    "    0: 'green',\n",
    "    1: 'palegreen',\n",
    "    2: 'papayawhip',\n",
    "    3: 'lightsalmon',\n",
    "    4: 'tomato'\n",
    "}\n",
    "\n",
    "vcol = [severity_cols[i] for i in df['Severity']]\n",
    "\n",
    "ax = plt.scatter(df['Start_Lng'], df['Start_Lat'],c = vcol,s=2)\n",
    "plt.title('Accidents representating map by severity level')\n",
    "fig = ax.get_figure()\n",
    "fig.savefig('Severity.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192e951f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# percentage of accident including road params, save as plot\n",
    "road_params = [\n",
    "    'Amenity', \n",
    "    'Bump', \n",
    "    'Crossing',\n",
    "    'Give_Way', \n",
    "    'Junction', \n",
    "    'No_Exit',\n",
    "    'Railway', \n",
    "    'Roundabout', \n",
    "    'Station',\n",
    "    'Stop', \n",
    "    'Traffic_Calming',\n",
    "    'Traffic_Signal', \n",
    "    'Turning_Loop']\n",
    "\n",
    "# % of accident including road params\n",
    "road_param_percent = df.loc[:, road_params].sum() / len(df)\n",
    "plt.title('Presence of road element near accidents')\n",
    "plt.xlabel('% of total of accidents')\n",
    "ax=road_param_percent.sort_values().plot(kind='barh');\n",
    "\n",
    "fig = ax.get_figure()\n",
    "fig.savefig('road.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfeb3315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# percentage of accident by Weather_Condition\n",
    "acc_by_weather_condition = df.groupby('Weather_Condition').size() / len(df)\n",
    "acc_by_weather_condition = acc_by_weather_condition[acc_by_weather_condition > 0.005]\n",
    "plt.title('Presence of weather condition during accidents')\n",
    "plt.xlabel('% of total of accidents')\n",
    "acc_by_weather_condition.sort_values().plot(kind='barh');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c0cb83",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7117bc44",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc04bde9",
   "metadata": {},
   "source": [
    "## kmean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbe9d75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aeafb6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048e2280",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b53dd085",
   "metadata": {},
   "source": [
    "## DBSCAN\n",
    "### New york"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a05e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take lat and lng colunm filter Newyork data\n",
    "NY = df['City'] == 'New York'\n",
    "NY_df = df[NY]\n",
    "NY_loca_df = NY_df[['Start_Lat','Start_Lng']]\n",
    "NY_loca_df.columns = [\"latitude\", \"longitude\"]\n",
    "coords = NY_loca_df[[\"latitude\", \"longitude\"]]\n",
    "X = NY_loca_df.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16262c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#simply plot new york accident coordinate \n",
    "plt.scatter( NY_loca_df[\"longitude\"],NY_loca_df[\"latitude\"],s=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c7f514",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NearestNeighbors find knee point for optimal eps\n",
    "neigh = NearestNeighbors(n_neighbors=5)\n",
    "nbrs = neigh.fit(np.radians(X))\n",
    "distances, indices = nbrs.kneighbors(np.radians(X))\n",
    "distances = distances[:, 1]\n",
    "distances = np.sort(distances, axis=0)\n",
    "fig=plt.figure()\n",
    "plt.plot(distances)\n",
    "plt.xlim(4000, 5570)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe901142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first time try \n",
    "dbscan_cluster_model = DBSCAN(eps=0.000035, min_samples=5, algorithm='ball_tree', metric='haversine').fit(np.radians(X))\n",
    "dbscan_cluster_model\n",
    "dbscan_cluster_model.labels_\n",
    "NY_loca_df['cluster'] = dbscan_cluster_model.labels_\n",
    "location = NY_loca_df['latitude'].mean(), NY_loca_df['longitude'].mean()\n",
    "\n",
    "m = folium.Map(location=location,zoom_start=11,control_scale = True)\n",
    "\n",
    "folium.TileLayer('cartodbpositron').add_to(m)\n",
    "\n",
    "clust_colours = ['#a6cee3','#1f78b4','#b2df8a','#33a02c','#fb9a99','#e31a1c','#fdbf6f','#ff7f00','#cab2d6','#6a3d9a','#ffff99','#b15928']\n",
    "\n",
    "for i in range(0,len(NY_loca_df)):\n",
    "    colouridx = NY_loca_df['cluster'].iloc[i]\n",
    "    if colouridx == -1:\n",
    "        pass\n",
    "    else:\n",
    "        col = clust_colours[colouridx%len(clust_colours)]\n",
    "        folium.CircleMarker([NY_loca_df['latitude'].iloc[i],NY_loca_df['longitude'].iloc[i]], radius = 10, color = col, fill = col).add_to(m)\n",
    "\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae30de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find optimal min_samples based on \n",
    "\n",
    "ss(X, NY_loca_df['cluster'])\n",
    "epsilons = np.linspace(6.75e-05,6.75e-05, num=1)\n",
    "print(epsilons)\n",
    "min_samples = np.arange(2, 100 , step=5) \n",
    "print(min_samples)\n",
    "combinations = list(itertools.product(epsilons, min_samples))\n",
    "print(combinations)\n",
    "N = len(combinations)\n",
    "\n",
    "#define a function to run through all combinations\n",
    "def get_scores_and_labels(combinations, X):\n",
    "  scores = []\n",
    "  all_labels_list = []\n",
    "\n",
    "  for i, (eps, num_samples) in enumerate(combinations):\n",
    "    \n",
    "    dbscan_cluster_model = DBSCAN(eps= eps, min_samples= num_samples, algorithm='ball_tree', metric='haversine').fit(np.radians(X))\n",
    "    labels = dbscan_cluster_model.labels_\n",
    "    labels_set = set(labels)\n",
    "    num_clusters = len(labels_set)\n",
    "    if -1 in labels_set:\n",
    "      num_clusters -= 1\n",
    "    \n",
    "    if (num_clusters < 2) or (num_clusters > 50):\n",
    "      scores.append(-10)\n",
    "      all_labels_list.append('bad')\n",
    "      c = (eps, num_samples)\n",
    "      print(f\"Combination {c} on iteration {i+1} of {N} has {num_clusters} clusters. Moving on\")\n",
    "      continue\n",
    "    \n",
    "    scores.append(ss(X, labels))\n",
    "    all_labels_list.append(labels)\n",
    "    print(f\"Index: {i}, Score: {scores[-1]}, Labels: {all_labels_list[-1]}, NumClusters: {num_clusters}\")\n",
    "\n",
    "  best_index = np.argmax(scores)\n",
    "  best_parameters = combinations[best_index]\n",
    "  best_labels = all_labels_list[best_index]\n",
    "  best_score = scores[best_index]\n",
    "\n",
    "  return {'best_epsilon': best_parameters[0],\n",
    "          'best_min_samples': best_parameters[1], \n",
    "          'best_labels': best_labels,\n",
    "          'best_score': best_score}\n",
    "\n",
    "# find best model\n",
    "best_dict = get_scores_and_labels(combinations, X)\n",
    "NY_loca_df['cluster'] = best_dict['best_labels']\n",
    "best_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a5c128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick clustered data excluding outliers\n",
    "involved = NY_loca_df['cluster'] != -1\n",
    "NY_involved = NY_loca_df[involved]\n",
    "NY_involved\n",
    "\n",
    "Xx = NY_involved[['latitude','longitude']].to_numpy()\n",
    "lablel = NY_involved[['cluster']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940c2d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get clusters centoid \n",
    "num_clusters = len(set(best_dict['best_labels']) - set([-1]))\n",
    "cluster_labels = best_dict['best_labels']\n",
    "clusters = pd.Series([X[cluster_labels == n] for n in range(num_clusters)])\n",
    "clusters\n",
    "\n",
    "#clusters centoid \n",
    "def get_centermost_point(cluster):\n",
    "    centroid = (MultiPoint(cluster).centroid.x, MultiPoint(cluster).centroid.y)\n",
    "    centermost_point = min(cluster, key=lambda point: great_circle(point, centroid).m)\n",
    "    return tuple(centermost_point)\n",
    "\n",
    "# get the centroid point for each cluster\n",
    "centermost_points = clusters.map(get_centermost_point)\n",
    "lats, lons = zip(*centermost_points)\n",
    "rep_points = pd.DataFrame({'lon':lons, 'lat':lats})\n",
    "rep_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae447e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# map all data by folium and save the map\n",
    "location = NY_loca_df['latitude'].mean(), NY_loca_df['longitude'].mean()\n",
    "\n",
    "m = folium.Map(location=location,zoom_start=11,control_scale = True)\n",
    "folium.TileLayer('cartodbpositron').add_to(m)\n",
    "\n",
    "\n",
    "for i in range(0,len(NY_loca_df)):\n",
    "    colouridx = NY_loca_df['cluster'].iloc[i]\n",
    "    if colouridx == -1:\n",
    "        folium.CircleMarker([NY_loca_df['latitude'].iloc[i],NY_loca_df['longitude'].iloc[i]], radius = 5, color = \"white\", fill = \"white\").add_to(m)\n",
    "    else:\n",
    "        col = clust_colours[colouridx%len(clust_colours)]\n",
    "        folium.CircleMarker([NY_loca_df['latitude'].iloc[i],NY_loca_df['longitude'].iloc[i]], radius = 5, color = col, fill = col).add_to(m)\n",
    "        \n",
    "for i in range(len(rep_points)):\n",
    "    folium.CircleMarker([rep_points['lat'].iloc[i],rep_points['lon'].iloc[i]], radius = 2, color = \"black\", fill_opacity=0.7, fill = \"black\").add_to(m)      \n",
    "        \n",
    "m.save(\"ny.html\")\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05595f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# davies_bouldin_score\n",
    "db_index = davies_bouldin_score(X, best_dict['best_labels'])\n",
    "db_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2b3f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ch_index\n",
    "ch_index = calinski_harabasz_score(X, best_dict['best_labels'])\n",
    "print(ch_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ce4019",
   "metadata": {},
   "source": [
    "### US\n",
    "the codes are very same as what we did on NY data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc67c29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take lat and lng colunm \n",
    "US_loca_df = df[['Start_Lat','Start_Lng']]\n",
    "US_loca_df.columns = [\"latitude\", \"longitude\"]\n",
    "coords = US_loca_df[[\"latitude\", \"longitude\"]]\n",
    "X = US_loca_df.to_numpy()\n",
    "\n",
    "# random select 100000 samples from entire US data\n",
    "l = list(range(2243939))\n",
    "random.seed(10)\n",
    "pick = sorted(random.sample(l, 100000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b196a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "##simply plot US accident coordinate \n",
    "plt.scatter( US_picked_df[\"longitude\"],US_picked_df[\"latitude\"],s=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faad1faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#elbow method define range of eps\n",
    "neigh = NearestNeighbors(n_neighbors=300)\n",
    "nbrs = neigh.fit(np.radians(X))\n",
    "distances, indices = nbrs.kneighbors(np.radians(X))\n",
    "distances = distances[:, 1]\n",
    "distances = np.sort(distances, axis=0)\n",
    "fig=plt.figure()\n",
    "plt.plot(distances)\n",
    "plt.xlim(99000, 100300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c531841f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "dbscan_cluster_model = DBSCAN(eps=0.009, min_samples=605, algorithm='ball_tree', metric='haversine').fit(np.radians(X))\n",
    "dbscan_cluster_model\n",
    "dbscan_cluster_model.labels_\n",
    "US_picked_df['cluster'] = dbscan_cluster_model.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54dcb30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# score of test\n",
    "ss(X, US_picked_df['cluster'])\n",
    "# below is testing range of min_s and eps\n",
    "\n",
    "epsilons = np.linspace(0.009,0.011, num=3)\n",
    "min_samples = np.arange(100, 700, step=55) \n",
    "combinations = list(itertools.product(epsilons, min_samples))\n",
    "combinations\n",
    "N = len(combinations)\n",
    "# find best model\n",
    "def get_scores_and_labels(combinations, X):\n",
    "  scores = []\n",
    "  all_labels_list = []\n",
    "  \n",
    "\n",
    "  for i, (eps, num_samples) in enumerate(combinations):\n",
    "    \n",
    "    dbscan_cluster_model = DBSCAN(eps= eps, min_samples= num_samples, algorithm='ball_tree', metric='haversine').fit(np.radians(X))\n",
    "    labels = dbscan_cluster_model.labels_\n",
    "    labels_set = set(labels)\n",
    "    num_clusters = len(labels_set)\n",
    "    if -1 in labels_set:\n",
    "      num_clusters -= 1\n",
    "    \n",
    "    if (num_clusters < 2) or (num_clusters > 50):\n",
    "      scores.append(-10)\n",
    "      all_labels_list.append('bad')\n",
    "      c = (eps, num_samples)\n",
    "      print(f\"Combination {c} on iteration {i+1} of {N} has {num_clusters} clusters. Moving on\")\n",
    "      continue\n",
    "    \n",
    "    scores.append(ss(X, labels))\n",
    "    all_labels_list.append(labels)\n",
    "    print(f\"Index: {i}, Score: {scores[-1]}, Labels: {all_labels_list[-1]}, NumClusters: {num_clusters}\")\n",
    "\n",
    "  best_index = np.argmax(scores)\n",
    "  best_parameters = combinations[best_index]\n",
    "  best_labels = all_labels_list[best_index]\n",
    "  best_score = scores[best_index]\n",
    "\n",
    "  return {'best_epsilon': best_parameters[0],\n",
    "          'best_min_samples': best_parameters[1], \n",
    "          'best_labels': best_labels,\n",
    "          'best_score': best_score}\n",
    "\n",
    "best_dict = get_scores_and_labels(combinations, X)\n",
    "US_picked_df['cluster'] = best_dict['best_labels']\n",
    "best_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477de0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find centroid for each cluster\n",
    "involved = US_picked_df['cluster'] != -1\n",
    "US_involved = US_picked_df[involved]\n",
    "US_involved\n",
    "\n",
    "\n",
    "Xx = US_involved[['latitude','longitude']].to_numpy()\n",
    "lablel = US_involved[['cluster']]\n",
    "num_clusters = len(set(best_dict['best_labels']) - set([-1]))\n",
    "cluster_labels = best_dict['best_labels']\n",
    "clusters = pd.Series([X[cluster_labels == n] for n in range(num_clusters)])\n",
    "clusters\n",
    "\n",
    "def get_centermost_point(cluster):\n",
    "    centroid = (MultiPoint(cluster).centroid.x, MultiPoint(cluster).centroid.y)\n",
    "    centermost_point = min(cluster, key=lambda point: great_circle(point, centroid).m)\n",
    "    return tuple(centermost_point)\n",
    "\n",
    "# get the centroid point for each cluster\n",
    "centermost_points = clusters.map(get_centermost_point)\n",
    "lats, lons = zip(*centermost_points)\n",
    "rep_points = pd.DataFrame({'lon':lons, 'lat':lats})\n",
    "rep_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27536591",
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualisation \n",
    "location = US_picked_df['latitude'].mean(), US_picked_df['longitude'].mean()\n",
    "\n",
    "m = folium.Map(location=location,zoom_start=4,control_scale = True)\n",
    "\n",
    "folium.TileLayer('cartodbpositron').add_to(m)\n",
    "\n",
    "clust_colours = ['#a6cee3','#1f78b4','#b2df8a','#33a02c','#fb9a99','#e31a1c','#fdbf6f','#ff7f00','#cab2d6','#6a3d9a','#ffff99','#b15928']\n",
    "\n",
    "for i in range(0,len(US_picked_df)):\n",
    "    colouridx = US_picked_df['cluster'].iloc[i]\n",
    "    if colouridx != -1:\n",
    "         folium.CircleMarker([US_picked_df['latitude'].iloc[i],US_picked_df['longitude'].iloc[i]], radius = 4, color = \"white\", fill = \"white\").add_to(m)\n",
    "         \n",
    "         col = clust_colours[colouridx%len(clust_colours)]\n",
    "         folium.CircleMarker([US_picked_df['latitude'].iloc[i],US_picked_df['longitude'].iloc[i]], radius = 4, color = col, fill = col).add_to(m)\n",
    "        \n",
    "    else:\n",
    "       \n",
    "        folium.CircleMarker([US_picked_df['latitude'].iloc[i],US_picked_df['longitude'].iloc[i]], radius = 4, color = \"white\", fill = \"white\").add_to(m)\n",
    "for i in range(len(rep_points)):\n",
    "    folium.CircleMarker([rep_points['lat'].iloc[i],rep_points['lon'].iloc[i]], radius = 3, color = \"black\", fill = \"black\").add_to(m)      \n",
    "        \n",
    "m\n",
    "m.save(\"US.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bcc2ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#davies_bouldin_score and calinski_harabasz_score\n",
    "db_index = davies_bouldin_score(X, best_dict['best_labels'])\n",
    "print(db_index)\n",
    "ch_index = calinski_harabasz_score(X, best_dict['best_labels'])\n",
    "print(ch_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326154d4",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb77f7a",
   "metadata": {},
   "source": [
    "# Text Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbf1439",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6e66c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0544ad3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "567a1afe",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531fa6b0",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28494fb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd3dbc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8067f91f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
