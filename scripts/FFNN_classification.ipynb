{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2393430d",
   "metadata": {
    "id": "2393430d"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "import sklearn.linear_model as lm\n",
    "import pandas as pd\n",
    "from matplotlib.pylab import (figure, semilogx, loglog, xlabel, ylabel, legend, \n",
    "                           title, subplot, show, grid)\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, mean_absolute_error, confusion_matrix\n",
    "import pickle\n",
    "from scipy.linalg import svd\n",
    "import seaborn as sn\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5kLnsFNDelI_",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5kLnsFNDelI_",
    "outputId": "28c8887f-036e-421d-df6d-5831391b7b7c"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive #For google colab integration\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ZQ79KrtAZii",
   "metadata": {
    "id": "7ZQ79KrtAZii"
   },
   "source": [
    "First we load the data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yHl0LeLyemGY",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yHl0LeLyemGY",
    "outputId": "270295b1-7b2c-4b05-e126-80b99909aebd"
   },
   "outputs": [],
   "source": [
    "!pip3 install pickle5\n",
    "import pickle5 as pickle\n",
    "# df = pd.read_csv('/content/drive/MyDrive/Computational Tools/final_df234.csv')\n",
    "with open('/content/drive/MyDrive/Computational_Tools/final_df234.pickle', 'rb') as f:\n",
    "  df = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yO95LObl7i5e",
   "metadata": {
    "id": "yO95LObl7i5e"
   },
   "outputs": [],
   "source": [
    "n = 1500000 #Sample size\n",
    "df_subset = df.sample(n).copy()\n",
    "#split into data and targets:\n",
    "X = df_subset.drop(columns = ['Severity']).copy() \n",
    "y = df_subset['Severity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e24e428",
   "metadata": {
    "id": "6e24e428"
   },
   "outputs": [],
   "source": [
    "#Normalize data\n",
    "normalized_df=(X-X.mean())/X.std()\n",
    "\n",
    "#Split data into training(95%), validation(2.5%) and test split(2.5%):\n",
    "X_train, X_rem, y_train, y_rem = train_test_split(X,y,train_size = 0.95)\n",
    "\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_rem,y_rem, test_size=0.5)\n",
    "\n",
    "#the data set contains far more datapoints of severity 2 then the other values, making severity 2 the most common category. \n",
    "#To avoid this messing with the outcome, we try undersampling classes Severity 2 and 3, and oversampling category 1.\n",
    "train_data = pd.concat([X_train, y_train], axis=1)\n",
    "\n",
    "#Separate the dataset by class\n",
    "Sev1 = train_data[train_data.Severity==1]\n",
    "Sev2 = train_data[train_data.Severity==2]\n",
    "Sev3 = train_data[train_data.Severity==3]\n",
    "Sev4 = train_data[train_data.Severity==4]\n",
    "Majority_list = [Sev1,Sev2,Sev3]\n",
    "#Set over and undersampling ratios\n",
    "# over_under_size = [int(a*(len(Sev4)/50000)) for a in [10000,100000,70000]]\n",
    "over_under_size = [len(Sev4),len(Sev4),len(Sev4)]\n",
    "downsampled = Sev4\n",
    "for i in range(len(Majority_list)):\n",
    "  Maj_downsampled = resample(Majority_list[i], replace=True,  n_samples=over_under_size[i], random_state=27) #resample data\n",
    "  downsampled = pd.concat([downsampled,Maj_downsampled],axis=0)\n",
    "#Set data to tensors of correct datatype:\n",
    "X_train = torch.Tensor(downsampled.drop(columns = [\"Severity\"]).copy().values).type(torch.float64)\n",
    "y_train = torch.Tensor(downsampled['Severity'].values).type(torch.LongTensor)-1\n",
    "X_test = torch.Tensor(X_test.values).type(torch.float64)\n",
    "X_val = torch.Tensor(X_val.values).type(torch.float64)\n",
    "y_test = torch.Tensor(y_test.values)-1\n",
    "y_val = torch.Tensor(y_val.values)-1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "N0XLC3L9hmoI",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N0XLC3L9hmoI",
    "outputId": "97994764-b17a-4d1f-c804-be7e56960e85"
   },
   "outputs": [],
   "source": [
    "#Check the size of the training and validation set:\n",
    "print(len(X_train),len(y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "QdxGzR20q0jI",
   "metadata": {
    "id": "QdxGzR20q0jI"
   },
   "source": [
    "We can check if the data can predict the class label with a simple linear regression. We do this to get an idea of how difficult the problem is to solve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Og3f8cHWfBHc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 302
    },
    "id": "Og3f8cHWfBHc",
    "outputId": "ef7ae6a6-e7a3-4318-a0c2-335a90f47802"
   },
   "outputs": [],
   "source": [
    "#We use an sklearn implementation of linear regression since we only want to check if the problem is easily solvable:\n",
    "model = lm.LinearRegression()\n",
    "model.fit(X_train,y_train)\n",
    "\n",
    "# Predict alcohol content\n",
    "y_est = model.predict(X_test)\n",
    "residual = y_est-y_test.numpy()\n",
    "\n",
    "# Display scatter plot\n",
    "figure()\n",
    "subplot(2,1,1)\n",
    "plt.plot(y_test, y_est, '.')\n",
    "plt.xlabel('Severity (true)'); plt.ylabel('Severity (estimated)');\n",
    "  \n",
    "subplot(2,1,2)\n",
    "plt.hist(residual,bins=[0,0.5,1.5,2.5,3.5,4])\n",
    "\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yAHHO_iirG2B",
   "metadata": {
    "id": "yAHHO_iirG2B"
   },
   "source": [
    "The problem is not easily solved, since the linear regression cannot distinguish between the classes\n",
    "\n",
    "Below we build the network(adapted from 02456 exercise 3.3 2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "D6avV4chZsM2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D6avV4chZsM2",
    "outputId": "4b3ec50d-d89c-4d54-c525-816f5cf9d37e"
   },
   "outputs": [],
   "source": [
    "#Hyperparameters\n",
    "num_output = 4\n",
    "num_l1 = 1024\n",
    "num_l2 = 2048\n",
    "num_features = X_train.shape[1]\n",
    "\n",
    "# define network\n",
    "class Net(nn.Module):\n",
    "  def __init__(self,num_features, num_hidden, num_hidden_2, num_output):\n",
    "    super(Net, self).__init__()\n",
    "    self.linear1 = nn.Linear(num_features,num_l1)\n",
    "    self.linear2 = nn.Linear(num_l1,num_l2)\n",
    "    self.linear3 = nn.Linear(num_l2,num_l1)\n",
    "    self.linear4 = nn.Linear(num_l1,num_output)\n",
    "    # self.linear3 = nn.Linear(num_l2,num_output)\n",
    "\n",
    "\n",
    "    # self.activation = torch.nn.Tanh()\n",
    "    self.activation = torch.nn.ReLU()\n",
    "    self.dropout = torch.nn.Dropout(p=0.6)\n",
    "    self.batchnorm1 = torch.nn.BatchNorm1d(num_hidden)\n",
    "    self.batchnorm2 = torch.nn.BatchNorm1d(num_hidden_2)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.linear1(x)\n",
    "    x = self.dropout(x)\n",
    "    x = self.activation(x)\n",
    "    x = self.batchnorm1(x)\n",
    "    x = self.linear2(x)\n",
    "    x = self.dropout(x)\n",
    "    x = self.activation(x)\n",
    "    x = self.batchnorm2(x)\n",
    "    x = self.linear3(x)\n",
    "    x = self.dropout(x)\n",
    "    x = self.batchnorm1(x)\n",
    "    x = self.linear4(x)\n",
    "    return F.softmax(x,dim=1)\n",
    "\n",
    "net = Net(num_features, num_l1, num_l2, num_output).double()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print('##converting network to cuda-enabled')\n",
    "    net.cuda()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e513161f",
   "metadata": {
    "id": "e513161f"
   },
   "outputs": [],
   "source": [
    "#Define the optimizer and criterion(Loss function)\n",
    "optimizer = optim.Adam(net.parameters(), lr=1e-4, weight_decay = 1e-5)\n",
    "criterion = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "utrDsyz8qgf9",
   "metadata": {
    "id": "utrDsyz8qgf9"
   },
   "source": [
    "Below we have the training loop for the network (adapted from 02456 exercise 3.3)\n",
    "In order to run this code, cuda will have to be available on your machine. Otherwise it can run on google colab using a GPU runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed87fd5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 287
    },
    "id": "1ed87fd5",
    "outputId": "db0f803b-ee9c-4e39-c1c1-7a4b68f362d6"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# setting hyperparameters and gettings epoch sizes\n",
    "batch_size = 4096\n",
    "num_epochs = 100\n",
    "num_samples_train = X_train.shape[0]\n",
    "num_batches_train = num_samples_train // batch_size\n",
    "num_samples_valid = X_val.shape[0]\n",
    "num_batches_valid = num_samples_valid // batch_size\n",
    "\n",
    "# setting up lists for handling loss/accuracy\n",
    "train_acc, train_loss = [], []\n",
    "valid_acc, valid_loss = [], []\n",
    "test_acc, test_loss = [], []\n",
    "cur_loss = 0\n",
    "losses = []\n",
    "\n",
    "get_slice = lambda i, size: range(i * size, (i + 1) * size)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward -> Backprob -> Update params\n",
    "    ## Train\n",
    "    cur_loss = 0\n",
    "    net.train()\n",
    "    for i in range(num_batches_train):\n",
    "\n",
    "\n",
    "        slce = get_slice(i, batch_size)\n",
    "        input = X_train[slce].cuda()\n",
    "        # input = input.cuda()\n",
    "        output = net(input)\n",
    "        \n",
    "        # compute gradients given loss\n",
    "        target_batch = y_train[slce]\n",
    "        target_batch = target_batch.cuda()\n",
    "        batch_loss = criterion(output, target_batch)\n",
    "        # batch_loss.requires_grad=True\n",
    "        optimizer.zero_grad()\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        cur_loss += batch_loss  \n",
    "    losses.append(cur_loss / batch_size)\n",
    "\n",
    "    net.eval()\n",
    "    ### Evaluate training\n",
    "    train_preds, train_targs = [], []\n",
    "    for i in range(num_batches_train):\n",
    "        slce = get_slice(i, batch_size)\n",
    "        input = X_train[slce].cuda()\n",
    "        output = net(input)\n",
    "        \n",
    "        preds = torch.max(output, 1)[1]\n",
    "        \n",
    "        train_targs += list(y_train[slce].cpu().numpy())\n",
    "        train_preds += list(preds.data.cpu().numpy())\n",
    "    \n",
    "    ### Evaluate validation\n",
    "    val_preds, val_targs = [], []\n",
    "    for i in range(num_batches_valid):\n",
    "        slce = get_slice(i, batch_size)\n",
    "        input = X_val[slce].cuda()\n",
    "        output = net(input)\n",
    "        preds = torch.max(output, 1)[1]\n",
    "        val_targs += list(y_val[slce].cpu().numpy())\n",
    "        val_preds += list(preds.data.cpu().numpy())\n",
    "        \n",
    "\n",
    "    train_acc_cur = accuracy_score(train_targs, train_preds)\n",
    "    valid_acc_cur = accuracy_score(val_targs, val_preds)\n",
    "    \n",
    "    train_acc.append(train_acc_cur)\n",
    "    valid_acc.append(valid_acc_cur)\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(\"Epoch %2i : Train Loss %f , Train acc %f, Valid acc %f\" % (\n",
    "                epoch+1, losses[-1], train_acc_cur, valid_acc_cur))\n",
    "\n",
    "epoch = np.arange(len(train_acc))\n",
    "plt.figure()\n",
    "plt.plot(epoch, train_acc, 'r', epoch, valid_acc, 'b')\n",
    "plt.legend(['Train Accucary','Validation Accuracy'])\n",
    "plt.xlabel('Updates'), plt.ylabel('Acc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Z84eR-qfrVgK",
   "metadata": {
    "id": "Z84eR-qfrVgK"
   },
   "source": [
    "Please not that due to the randomness involved in sampling data points, splitting the dataset and initializing parameters, results of training may not be exactly the same as reported in the paper.\n",
    "\n",
    "Below we plot the loss curve for the training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vMFIFywmNvj8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "id": "vMFIFywmNvj8",
    "outputId": "b9144e4c-5ea9-42a6-afcf-6e7a7a206098"
   },
   "outputs": [],
   "source": [
    "losses = [loss.cpu().detach().numpy() for loss in losses]\n",
    "plt.figure()\n",
    "plt.plot(epoch, losses)\n",
    "plt.legend(['Loss'])\n",
    "plt.xlabel('Updates') #, plt.ylabel('')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uZQe2KJ-rawL",
   "metadata": {
    "id": "uZQe2KJ-rawL"
   },
   "source": [
    "Then we can plot a confusion matrix to tell how our model performed, and in what mistakes the network made, if any."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2weTbxiSrjS5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 461
    },
    "id": "2weTbxiSrjS5",
    "outputId": "b3be9525-eac4-440c-a816-e74a51b3691f"
   },
   "outputs": [],
   "source": [
    "net.cpu()\n",
    "preds = torch.max(net(X_test), 1)[1]\n",
    "cm = confusion_matrix(y_test,preds)\n",
    "df_cm = pd.DataFrame(cm, index = range(1,5), columns = range(1,5))\n",
    "plt.figure(figsize=(10,7))\n",
    "sn.heatmap(df_cm, annot=True)\n",
    "plt.title('Test Accuracy: {}'.format(accuracy_score(y_test,preds)))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
